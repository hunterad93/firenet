{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function description:\n",
    "\n",
    "1. Select appropriate list of blobs from gcp fs, most recent hour's worth of data from GOES MCMIPC bucket. This should be 12 blobs because there are scans every 5 mins.\n",
    "2. Download the set of blobs, pruning unneeded data such as data quality flag arrays and unused bands, return list of data sets\n",
    "3. Concatenate the 12 datasets into one, effectively creating a dataset with a time dimension\n",
    "4. Take median over the time dimension, so each pixel has median value of the last hour for each band\n",
    "5. Feature engineer the median dataset, adding more informative bands that are ratios of the spectral channels\n",
    "6. Reproject this dataset to epsg 4326\n",
    "7. Download the preprocessed landfire layers. These have been reproject_matched to a GOES CONUS 'template' image, which has itself been reprojected to epsg 4326. This is intended to match the slightly convoluted preprocessing routine of training data the pytorch model was trained on.\n",
    "8. Stack the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "9. Chunk the stacks to pytorch manageable size and upload to a bucket, this will be a large list of dataset files that have the stacked raster imagery with metadata that can be used to project pytorch container's inference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import os\n",
    "import tempfile\n",
    "from rasterio.enums import Resampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_blobs(bucket_name='gcp-public-data-goes-16'):\n",
    "    \"\"\"\n",
    "    Selects the appropriate list of blobs from GCP fs, most recent hour's worth of data from GOES MCMIPC bucket.\n",
    "    Returns: List of selected blobs.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    attime = datetime.utcnow()\n",
    "\n",
    "    # Set up Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Create a range of directories to check. The GOES bucket is\n",
    "    # organized by hour of day.\n",
    "    selected_blobs = []\n",
    "    for i in range(2):  # Get blobs from current hour and previous hour\n",
    "        current_time = attime - timedelta(hours=i)\n",
    "        prefix = f'ABI-L2-MCMIPC/{current_time.year}/{current_time.timetuple().tm_yday:03d}/{current_time.hour:02d}/'\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        selected_blobs.extend([blob.name for blob in blobs])\n",
    "\n",
    "    # Sort the blobs by their timestamp in descending order\n",
    "    selected_blobs.sort(key=lambda name: name.split('_')[3][1:], reverse=True)\n",
    "\n",
    "    # Check if there are at least 12 blobs\n",
    "    if len(selected_blobs) < 12:\n",
    "        raise Exception(f\"Only {len(selected_blobs)} blobs found\")\n",
    "\n",
    "    return selected_blobs[:12]\n",
    "\n",
    "\n",
    "def create_fs():\n",
    "    \"\"\"\n",
    "    Creates a file system object for GCP. \n",
    "    Returns: File system object. fs can be interacted with as though it were a local file system.\n",
    "    \"\"\"\n",
    "    fs = fsspec.filesystem('gcs', token=os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "    return fs\n",
    "\n",
    "\n",
    "def create_median_image(blob_list, fs, bucket_name='gcp-public-data-goes-16'):\n",
    "    \"\"\"\n",
    "    This function creates a single-band image from a list of blob names.\n",
    "    The pixel values in the image are the median values from the corresponding pixels in the input images.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the Datasets\n",
    "    datasets = []\n",
    "\n",
    "    # Open each blob as a full dataset and load it into memory\n",
    "    for blob in blob_list[::6]:  # Use every 6th blob for testing purposes (faster operation creating each median without pipeline-structure difference), change to 1 later\n",
    "        f = fs.open(f'{bucket_name}/{blob}')\n",
    "        print(f'Opening: {bucket_name}/{blob}')\n",
    "        ds = xr.open_dataset(f).load()\n",
    "        datasets.append(ds)\n",
    "\n",
    "    # Concatenate the datasets along a new 'band' dimension\n",
    "    concated = xr.concat(datasets, dim='time')\n",
    "\n",
    "    # Compute the median along the 'band' dimension\n",
    "    median_ds = concated.median(dim='time', keep_attrs=True)\n",
    "\n",
    "    # Close the files\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "\n",
    "    # Return the median dataset\n",
    "    return median_ds\n",
    "\n",
    "def reproject_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Reprojects the dataset to epsg 5070 with 375 res.\n",
    "    Note that the technique used here is creating a tempfile and then opening it with rioxarray.\n",
    "    This is a hack but I could not find any other way to get rioxarray to recognize the spatial\n",
    "    metadata that is necessary for reprojection.\n",
    "    Returns: Reprojected dataset.\n",
    "    \"\"\"\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "        # Save the dataset to the temporary file\n",
    "        dataset.to_netcdf(tmpfile.name)\n",
    "\n",
    "        # Open the temporary file with rioxarray\n",
    "        ds_rio = rioxarray.open_rasterio(tmpfile.name)\n",
    "\n",
    "        # Reproject the dataset to CRS EPSG:5070\n",
    "        reprojected_dataset = ds_rio.rio.reproject(\"EPSG:5070\")\n",
    "\n",
    "    return reprojected_dataset\n",
    "\n",
    "def engineer_features(dataset):\n",
    "    \"\"\"\n",
    "    Feature engineers the median dataset, adding more informative bands that are ratios of the spectral channels.\n",
    "    Returns: Feature engineered dataset.\n",
    "    \"\"\"\n",
    "    # Compute the new features\n",
    "    feat1 = dataset['CMI_C06'] / dataset['CMI_C05']\n",
    "    feat2 = dataset['CMI_C07'] / dataset['CMI_C05']\n",
    "    feat3 = dataset['CMI_C07'] / dataset['CMI_C06']\n",
    "    feat4 = dataset['CMI_C14'] / dataset['CMI_C07']\n",
    "\n",
    "    # Create a dictionary of the new features\n",
    "    data_dict = {'6_5': feat1, '7_5': feat2, '7_6': feat3, '14_7': feat4}\n",
    "\n",
    "    # Add the new features to the dataset\n",
    "    engineered_dataset = dataset.assign(data_dict)\n",
    "\n",
    "    return engineered_dataset\n",
    "\n",
    "\n",
    "def download_landfire_layers():\n",
    "    \"\"\"\n",
    "    Downloads the preprocessed landfire layers. These have been reproject_matched to a GOES CONUS 'template' image, \n",
    "    which has itself been reprojected to epsg 4326.\n",
    "    Returns: Preprocessed landfire layers.\n",
    "    \"\"\"\n",
    "    # landfire_layers = ...\n",
    "    return landfire_layers\n",
    "\n",
    "def stack_datasets(goes_ds, landfire_layers):\n",
    "    \"\"\"\n",
    "    Stacks the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "    Returns: Stacked dataset.\n",
    "    \"\"\"\n",
    "    # Open the landfire layers dataset\n",
    "    with xr.open_dataset(landfire_layers) as landfire_ds:\n",
    "        # Merge the two datasets\n",
    "        stacked_dataset = xr.merge([goes_ds, landfire_ds])\n",
    "\n",
    "    return stacked_dataset\n",
    "\n",
    "def chunk_dataset(dataset):\n",
    "    # Get the width and height of the image\n",
    "    width = dataset.dims['x']\n",
    "    height = dataset.dims['y']\n",
    "    \n",
    "    # Calculate the number of chunks in x and y direction\n",
    "    nx, ny = width // 64, height // 64\n",
    "    \n",
    "    # Initialize a list to store the chunks\n",
    "    chunks = []\n",
    "    spatial_info = []\n",
    "    \n",
    "    # Loop over the image\n",
    "    for i in range(ny):\n",
    "        for j in range(nx):\n",
    "            # Define the slice\n",
    "            y_slice = slice(i * 64, (i + 1) * 64)\n",
    "            x_slice = slice(j * 64, (j + 1) * 64)\n",
    "    \n",
    "            # Extract the chunk across all bands\n",
    "            chunk = dataset.isel(y=y_slice, x=x_slice)\n",
    "    \n",
    "            # Store the chunk and its spatial information\n",
    "            chunks.append(chunk)\n",
    "            spatial_info.append((y_slice, x_slice))\n",
    "    \n",
    "    # Convert the chunks to a list of xarray Datasets\n",
    "    chunks = [chunk for chunk in chunks]\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_blobs = select_blobs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/023/05/OR_ABI-L2-MCMIPC-M6_G16_s20240230551170_e20240230553543_c20240230554064.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/023/05/OR_ABI-L2-MCMIPC-M6_G16_s20240230521170_e20240230523555_c20240230524073.nc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fs = create_fs()\n",
    "median_ds = create_median_image(selected_blobs, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_6975/1318561673.py:99: SerializationWarning: saving variable y with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n",
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_6975/1318561673.py:99: SerializationWarning: saving variable x with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3270.9038690767325, -3270.9038690767325)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_ds = engineer_features(median_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_6975/1318561673.py:99: SerializationWarning: saving variable y with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n",
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_6975/1318561673.py:99: SerializationWarning: saving variable x with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3270.9038690767325, -3270.9038690767325)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reprojected_median_ds = reproject_dataset(median_ds)\n",
    "reprojected_median_ds = engineer_features(reprojected_median_ds)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
