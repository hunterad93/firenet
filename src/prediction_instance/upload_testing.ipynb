{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geojson\n",
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_degrees(file_id):\n",
    "    \n",
    "    # Read in GOES ABI fixed grid projection variables and constants\n",
    "    x_coordinate_1d = file_id.variables['x'][:]  # E/W scanning angle in radians\n",
    "    y_coordinate_1d = file_id.variables['y'][:]  # N/S elevation angle in radians\n",
    "    projection_info = file_id.variables['goes_imager_projection']\n",
    "    lon_origin = projection_info.attrs['longitude_of_projection_origin']\n",
    "    H = projection_info.attrs['perspective_point_height'] + projection_info.attrs['semi_major_axis']\n",
    "    r_eq = projection_info.attrs['semi_major_axis']\n",
    "    r_pol = projection_info.attrs['semi_minor_axis']    \n",
    "    \n",
    "    # Create 2D coordinate matrices from 1D coordinate vectors\n",
    "    x_coordinate_2d, y_coordinate_2d = np.meshgrid(x_coordinate_1d, y_coordinate_1d)\n",
    "    \n",
    "    # Equations to calculate latitude and longitude\n",
    "    lambda_0 = (lon_origin*np.pi)/180.0  \n",
    "    a_var = np.power(np.sin(x_coordinate_2d),2.0) + (np.power(np.cos(x_coordinate_2d),2.0)*(np.power(np.cos(y_coordinate_2d),2.0)+(((r_eq*r_eq)/(r_pol*r_pol))*np.power(np.sin(y_coordinate_2d),2.0))))\n",
    "    b_var = -2.0*H*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    c_var = (H**2.0)-(r_eq**2.0)\n",
    "    r_s = (-1.0*b_var - np.sqrt((b_var**2)-(4.0*a_var*c_var)))/(2.0*a_var)\n",
    "    s_x = r_s*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    s_y = - r_s*np.sin(x_coordinate_2d)\n",
    "    s_z = r_s*np.cos(x_coordinate_2d)*np.sin(y_coordinate_2d)\n",
    "    \n",
    "    # Ignore numpy errors for sqrt of negative number; occurs for GOES-16 ABI CONUS sector data\n",
    "    np.seterr(all='ignore')\n",
    "    \n",
    "    abi_lat = (180.0/np.pi)*(np.arctan(((r_eq*r_eq)/(r_pol*r_pol))*((s_z/np.sqrt(((H-s_x)*(H-s_x))+(s_y*s_y))))))\n",
    "    abi_lon = (lambda_0 - np.arctan(s_y/(H-s_x)))*(180.0/np.pi)\n",
    "    print(abi_lat)\n",
    "    return abi_lat, abi_lon\n",
    "\n",
    "def get_blob_names(attime=datetime.utcnow(), within=pd.to_timedelta(\"1H\"), bucket_name='gcp-public-data-goes-16'):\n",
    "    if isinstance(attime, str):\n",
    "        attime = pd.to_datetime(attime)\n",
    "    if isinstance(within, str):\n",
    "        within = pd.to_timedelta(within)\n",
    "\n",
    "    # Parameter Setup\n",
    "    start = attime - within\n",
    "    end = attime + within\n",
    "\n",
    "    print(f\"Start: {start}, End: {end}\")\n",
    "\n",
    "    # Set up Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Create a range of directories to check. The GOES bucket is\n",
    "    # organized by hour of day.\n",
    "    blob_names = []\n",
    "    current_time = start\n",
    "    while current_time <= end:\n",
    "        prefix = f'ABI-L2-FDCC/{current_time.year}/{current_time.timetuple().tm_yday:03d}/{current_time.hour:02d}/'\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        blob_names.extend([blob.name for blob in blobs])\n",
    "        current_time += timedelta(hours=1)  # Increment current_time by 1 hour\n",
    "\n",
    "    print(blob_names)\n",
    "    return blob_names\n",
    "\n",
    "def select_blobs(blob_names):\n",
    "    # Sort blob names by timestamp\n",
    "    blob_names.sort(key=lambda name: name.split('_')[3][1:], reverse=True)  # Extract timestamp after 's'\n",
    "\n",
    "    # Extract band numbers from blob names\n",
    "    try:\n",
    "        band_numbers = [int(name.split('_')[1][-2:]) for name in blob_names]\n",
    "    except ValueError:\n",
    "        # If there is only one band and the band number is a string, return the most recent file\n",
    "        return [blob_names[0]]\n",
    "\n",
    "    # Get unique band numbers and sort them\n",
    "    unique_band_numbers = sorted(set(band_numbers))\n",
    "\n",
    "    # If there is only one unique band number, return the most recent file\n",
    "    if len(unique_band_numbers) == 1:\n",
    "        return [blob_names[0]]\n",
    "\n",
    "    # Find the first continuous sequence that matches the expected band order\n",
    "    for i in range(len(blob_names) - len(unique_band_numbers) + 1):\n",
    "        selected = blob_names[i:i+len(unique_band_numbers)]\n",
    "        band_order = [int(name.split('_')[1][-2:]) for name in selected]\n",
    "        if band_order == unique_band_numbers:\n",
    "            break\n",
    "    else:\n",
    "        raise Exception(\"No continuous sequence found that matches the expected band order\")\n",
    "    print(selected)\n",
    "    return selected\n",
    "\n",
    "def get_datasets(blob_names, fs, bucket_name='gcp-public-data-goes-16'):\n",
    "    # Open each blob as an xarray Dataset and store it in the dictionary under the corresponding channel identifier\n",
    "    datasets = {}\n",
    "    for name in blob_names:\n",
    "        channel_id = name.split('_')[1]\n",
    "        f = fs.open(f'{bucket_name}/{name}')\n",
    "        ds = xr.open_dataset(f, engine='h5netcdf')\n",
    "        datasets[channel_id] = ds\n",
    "    print(datasets)\n",
    "    return datasets\n",
    "\n",
    "def generate_geojson_points(ds):\n",
    "    # Process the data to generate GeoJSON\n",
    "    band_data = ds['DQF'].values\n",
    "    zero_indices = np.where(band_data == 0)\n",
    "    lat, lon = calculate_degrees(ds)\n",
    "    zero_lat_lon = lat[zero_indices], lon[zero_indices]\n",
    "    features = []\n",
    "    for lat, lon in zip(*zero_lat_lon):\n",
    "        point = geojson.Point((lon, lat))\n",
    "        features.append(geojson.Feature(geometry=point))\n",
    "    feature_collection = geojson.FeatureCollection(features)\n",
    "\n",
    "    # Convert the GeoJSON to a string\n",
    "    geojson_str = geojson.dumps(feature_collection)\n",
    "\n",
    "    # Extract the timestamp from the dataset\n",
    "    timestamp = ds.t.values\n",
    "\n",
    "    return timestamp, geojson_str\n",
    "\n",
    "def upload_to_bigquery(prediction_time, goesmask_geojson):\n",
    "    # Initialize a BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Specify your dataset and table\n",
    "    dataset_id = 'geojson_predictions'\n",
    "    table_id = 'goesmask'\n",
    "\n",
    "    # Get the table\n",
    "    table = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table)\n",
    "\n",
    "    # Convert numpy.datetime64 to datetime and then to string for bigquery\n",
    "    prediction_time = pd.to_datetime(str(prediction_time)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    # Prepare the row to be inserted\n",
    "    row = {\n",
    "        'prediction_date': prediction_time,\n",
    "        'goesmask_geojson': goesmask_geojson,\n",
    "    }\n",
    "\n",
    "    # Insert the row\n",
    "    errors = client.insert_rows_json(table, [row])\n",
    "\n",
    "    # Check if any errors occurred\n",
    "    if errors:\n",
    "        print('Errors:', errors)\n",
    "    else:\n",
    "        print('Row inserted successfully.')\n",
    "\n",
    "\n",
    "def GOES_GEOJSON_UPDATE(request):\n",
    "    # Use fsspec to create a file system\n",
    "    fs = fsspec.filesystem('gcs')\n",
    "    blob_names = get_blob_names()\n",
    "    selected_blobs = select_blobs(blob_names)\n",
    "    datasets = get_datasets(selected_blobs, fs)\n",
    "    if datasets:\n",
    "        first_ds_key = next(iter(datasets))\n",
    "    else:\n",
    "        # Handle the empty case, perhaps log an error or return\n",
    "        print(\"No datasets available.\")\n",
    "        return\n",
    "\n",
    "    first_ds = datasets[first_ds_key]\n",
    "\n",
    "    # Generate GeoJSON points from the first dataset\n",
    "    timestamp, geojson_str = generate_geojson_points(first_ds)\n",
    "    # Upload the generated GeoJSON to BigQuery\n",
    "    upload_to_bigquery(timestamp, geojson_str)\n",
    "\n",
    "    return 'Function executed successfully', 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/adamhunter/Documents/school projs/firenet/data/credentials/firenet-99-writer.json'\n",
    "# Use fsspec to create a file system\n",
    "fs = fsspec.filesystem('gcs', token=os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2023-12-10 23:18:30.117757, End: 2023-12-11 01:18:30.117757\n",
      "['ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442301176_e20233442303548_c20233442304294.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442306176_e20233442308549_c20233442309311.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442311176_e20233442313548_c20233442314326.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442316176_e20233442318548_c20233442319377.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442321176_e20233442323548_c20233442324334.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442326176_e20233442328548_c20233442329334.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442331176_e20233442333548_c20233442334301.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442336176_e20233442338548_c20233442339299.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442341176_e20233442343548_c20233442344187.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442346176_e20233442348548_c20233442349133.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442351176_e20233442353548_c20233442354124.nc', 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442356176_e20233442358548_c20233442359135.nc', 'ABI-L2-FDCC/2023/345/00/OR_ABI-L2-FDCC-M6_G16_s20233450001176_e20233450003548_c20233450004121.nc', 'ABI-L2-FDCC/2023/345/00/OR_ABI-L2-FDCC-M6_G16_s20233450006176_e20233450008548_c20233450009139.nc', 'ABI-L2-FDCC/2023/345/00/OR_ABI-L2-FDCC-M6_G16_s20233450011176_e20233450013548_c20233450014130.nc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442301176_e20233442303548_c20233442304294.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442306176_e20233442308549_c20233442309311.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442311176_e20233442313548_c20233442314326.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442316176_e20233442318548_c20233442319377.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442321176_e20233442323548_c20233442324334.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442326176_e20233442328548_c20233442329334.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442331176_e20233442333548_c20233442334301.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442336176_e20233442338548_c20233442339299.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442341176_e20233442343548_c20233442344187.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442346176_e20233442348548_c20233442349133.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442351176_e20233442353548_c20233442354124.nc',\n",
       " 'ABI-L2-FDCC/2023/344/23/OR_ABI-L2-FDCC-M6_G16_s20233442356176_e20233442358548_c20233442359135.nc',\n",
       " 'ABI-L2-FDCC/2023/345/00/OR_ABI-L2-FDCC-M6_G16_s20233450001176_e20233450003548_c20233450004121.nc',\n",
       " 'ABI-L2-FDCC/2023/345/00/OR_ABI-L2-FDCC-M6_G16_s20233450006176_e20233450008548_c20233450009139.nc',\n",
       " 'ABI-L2-FDCC/2023/345/00/OR_ABI-L2-FDCC-M6_G16_s20233450011176_e20233450013548_c20233450014130.nc']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attime = \"2023-12-10 12:00:00\"\n",
    "blob_names = get_blob_names()\n",
    "\n",
    "blob_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201356171_e20232201358544_c20232201359127.nc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_blobs = select_blobs(blob_names)\n",
    "selected_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_datasets() missing 1 required positional argument: 'fs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/adamhunter/Documents/school projs/firenet/src/prediction_instance/upload_testing.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adamhunter/Documents/school%20projs/firenet/src/prediction_instance/upload_testing.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m datasets \u001b[39m=\u001b[39m get_datasets(selected_blobs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamhunter/Documents/school%20projs/firenet/src/prediction_instance/upload_testing.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m datasets\n",
      "\u001b[0;31mTypeError\u001b[0m: get_datasets() missing 1 required positional argument: 'fs'"
     ]
    }
   ],
   "source": [
    "datasets = get_datasets(selected_blobs)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first dataset from the datasets dictionary\n",
    "first_ds_key = next(iter(datasets))\n",
    "first_ds = datasets[first_ds_key]\n",
    "\n",
    "# Generate GeoJSON points from the first dataset\n",
    "timestamp, geojson_str = generate_geojson_points(first_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Upload the generated GeoJSON to BigQuery\n",
    "upload_to_bigquery(timestamp, geojson_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
