{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geojson\n",
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_degrees(file_id):\n",
    "    \n",
    "    # Read in GOES ABI fixed grid projection variables and constants\n",
    "    x_coordinate_1d = file_id.variables['x'][:]  # E/W scanning angle in radians\n",
    "    y_coordinate_1d = file_id.variables['y'][:]  # N/S elevation angle in radians\n",
    "    projection_info = file_id.variables['goes_imager_projection']\n",
    "    lon_origin = projection_info.attrs['longitude_of_projection_origin']\n",
    "    H = projection_info.attrs['perspective_point_height'] + projection_info.attrs['semi_major_axis']\n",
    "    r_eq = projection_info.attrs['semi_major_axis']\n",
    "    r_pol = projection_info.attrs['semi_minor_axis']    \n",
    "    \n",
    "    # Create 2D coordinate matrices from 1D coordinate vectors\n",
    "    x_coordinate_2d, y_coordinate_2d = np.meshgrid(x_coordinate_1d, y_coordinate_1d)\n",
    "    \n",
    "    # Equations to calculate latitude and longitude\n",
    "    lambda_0 = (lon_origin*np.pi)/180.0  \n",
    "    a_var = np.power(np.sin(x_coordinate_2d),2.0) + (np.power(np.cos(x_coordinate_2d),2.0)*(np.power(np.cos(y_coordinate_2d),2.0)+(((r_eq*r_eq)/(r_pol*r_pol))*np.power(np.sin(y_coordinate_2d),2.0))))\n",
    "    b_var = -2.0*H*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    c_var = (H**2.0)-(r_eq**2.0)\n",
    "    r_s = (-1.0*b_var - np.sqrt((b_var**2)-(4.0*a_var*c_var)))/(2.0*a_var)\n",
    "    s_x = r_s*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    s_y = - r_s*np.sin(x_coordinate_2d)\n",
    "    s_z = r_s*np.cos(x_coordinate_2d)*np.sin(y_coordinate_2d)\n",
    "    \n",
    "    # Ignore numpy errors for sqrt of negative number; occurs for GOES-16 ABI CONUS sector data\n",
    "    np.seterr(all='ignore')\n",
    "    \n",
    "    abi_lat = (180.0/np.pi)*(np.arctan(((r_eq*r_eq)/(r_pol*r_pol))*((s_z/np.sqrt(((H-s_x)*(H-s_x))+(s_y*s_y))))))\n",
    "    abi_lon = (lambda_0 - np.arctan(s_y/(H-s_x)))*(180.0/np.pi)\n",
    "    \n",
    "    return abi_lat, abi_lon\n",
    "\n",
    "def get_blob_names(attime=datetime.utcnow(), within=pd.to_timedelta(\"1H\"), bucket_name='gcp-public-data-goes-16'):\n",
    "    if isinstance(attime, str):\n",
    "        attime = pd.to_datetime(attime)\n",
    "    if isinstance(within, str):\n",
    "        within = pd.to_timedelta(within)\n",
    "\n",
    "    # Parameter Setup\n",
    "    start = attime - within\n",
    "    end = attime + within\n",
    "\n",
    "    # Set up Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Create a range of directories to check. The GOES bucket is\n",
    "    # organized by hour of day.\n",
    "    blob_names = []\n",
    "    for hour in range(start.hour, end.hour + 1):\n",
    "        prefix = f'ABI-L2-FDCC/{start.year}/{start.timetuple().tm_yday:03d}/{hour:02d}/'\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        blob_names.extend([blob.name for blob in blobs])\n",
    "\n",
    "    return blob_names\n",
    "\n",
    "def select_blobs(blob_names):\n",
    "    # Sort blob names by timestamp\n",
    "    blob_names.sort(key=lambda name: name.split('_')[3][1:], reverse=True)  # Extract timestamp after 's'\n",
    "\n",
    "    # Extract band numbers from blob names\n",
    "    try:\n",
    "        band_numbers = [int(name.split('_')[1][-2:]) for name in blob_names]\n",
    "    except ValueError:\n",
    "        # If there is only one band and the band number is a string, return the most recent file\n",
    "        return [blob_names[0]]\n",
    "\n",
    "    # Get unique band numbers and sort them\n",
    "    unique_band_numbers = sorted(set(band_numbers))\n",
    "\n",
    "    # If there is only one unique band number, return the most recent file\n",
    "    if len(unique_band_numbers) == 1:\n",
    "        return [blob_names[0]]\n",
    "\n",
    "    # Find the first continuous sequence that matches the expected band order\n",
    "    for i in range(len(blob_names) - len(unique_band_numbers) + 1):\n",
    "        selected = blob_names[i:i+len(unique_band_numbers)]\n",
    "        band_order = [int(name.split('_')[1][-2:]) for name in selected]\n",
    "        if band_order == unique_band_numbers:\n",
    "            break\n",
    "    else:\n",
    "        raise Exception(\"No continuous sequence found that matches the expected band order\")\n",
    "\n",
    "    return selected\n",
    "\n",
    "def get_datasets(blob_names, bucket_name='gcp-public-data-goes-16'):\n",
    "    # Open each blob as an xarray Dataset and store it in the dictionary under the corresponding channel identifier\n",
    "    datasets = {}\n",
    "    for name in blob_names:\n",
    "        channel_id = name.split('_')[1]\n",
    "        f = fs.open(f'{bucket_name}/{name}')\n",
    "        ds = xr.open_dataset(f, engine='h5netcdf')\n",
    "        datasets[channel_id] = ds\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def generate_geojson_points(ds):\n",
    "    # Process the data to generate GeoJSON\n",
    "    band_data = ds['DQF'].values\n",
    "    zero_indices = np.where(band_data == 0)\n",
    "    lat, lon = calculate_degrees(ds)\n",
    "    zero_lat_lon = lat[zero_indices], lon[zero_indices]\n",
    "    features = []\n",
    "    for lat, lon in zip(*zero_lat_lon):\n",
    "        point = geojson.Point((lon, lat))\n",
    "        features.append(geojson.Feature(geometry=point))\n",
    "    feature_collection = geojson.FeatureCollection(features)\n",
    "\n",
    "    # Convert the GeoJSON to a string\n",
    "    geojson_str = geojson.dumps(feature_collection)\n",
    "\n",
    "    # Extract the timestamp from the dataset\n",
    "    timestamp = ds.t.values\n",
    "\n",
    "    return timestamp, geojson_str\n",
    "\n",
    "def upload_to_bigquery(prediction_time, goesmask_geojson):\n",
    "    # Initialize a BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Specify your dataset and table\n",
    "    dataset_id = 'geojson_predictions'\n",
    "    table_id = 'goesmask_and_unet'\n",
    "\n",
    "    # Get the table\n",
    "    table = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table)\n",
    "\n",
    "    # Convert numpy.datetime64 to datetime and then to string for bigquery\n",
    "    prediction_time = pd.to_datetime(str(prediction_time)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    # Prepare the row to be inserted\n",
    "    row = {\n",
    "        'prediction_date': prediction_time,\n",
    "        'goes_mask': goesmask_geojson,\n",
    "        'ea_unet': None  # This will be NULL in BigQuery\n",
    "    }\n",
    "\n",
    "    # Insert the row\n",
    "    errors = client.insert_rows_json(table, [row])\n",
    "\n",
    "    # Check if any errors occurred\n",
    "    if errors:\n",
    "        print('Errors:', errors)\n",
    "    else:\n",
    "        print('Row inserted successfully.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/adamhunter/Documents/school projs/firenet/data/credentials/firenet-99-writer.json'\n",
    "# Use fsspec to create a file system\n",
    "fs = fsspec.filesystem('gcs', token=os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201101171_e20232201103544_c20232201104221.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201106171_e20232201108544_c20232201109182.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201111171_e20232201113544_c20232201114154.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201116171_e20232201118544_c20232201119209.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201121171_e20232201123544_c20232201124162.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201126171_e20232201128544_c20232201129171.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201131171_e20232201133544_c20232201134158.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201136171_e20232201138544_c20232201139166.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201141171_e20232201143544_c20232201144151.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201146171_e20232201148544_c20232201149151.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201151171_e20232201153544_c20232201154133.nc',\n",
       " 'ABI-L2-FDCC/2023/220/11/OR_ABI-L2-FDCC-M6_G16_s20232201156171_e20232201158544_c20232201159131.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201201171_e20232201203544_c20232201204161.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201206171_e20232201208544_c20232201209136.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201211171_e20232201213544_c20232201214136.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201216171_e20232201218544_c20232201219150.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201221171_e20232201223544_c20232201224127.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201226171_e20232201228544_c20232201229134.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201231171_e20232201233544_c20232201234142.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201236171_e20232201238544_c20232201239123.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201241171_e20232201243544_c20232201244129.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201246171_e20232201248544_c20232201249155.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201251171_e20232201253544_c20232201254125.nc',\n",
       " 'ABI-L2-FDCC/2023/220/12/OR_ABI-L2-FDCC-M6_G16_s20232201256171_e20232201258544_c20232201259132.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201301171_e20232201303544_c20232201304137.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201306171_e20232201308544_c20232201309129.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201311171_e20232201313544_c20232201314126.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201316171_e20232201318544_c20232201319168.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201321171_e20232201323544_c20232201324120.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201326171_e20232201328544_c20232201329155.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201331171_e20232201333544_c20232201334147.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201336171_e20232201338544_c20232201339123.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201341171_e20232201343544_c20232201344132.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201346171_e20232201348544_c20232201349140.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201351171_e20232201353544_c20232201354115.nc',\n",
       " 'ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201356171_e20232201358544_c20232201359127.nc']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attime = \"2023-08-08 12:00:00\"\n",
    "blob_names = get_blob_names(attime=attime)\n",
    "\n",
    "blob_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABI-L2-FDCC/2023/220/13/OR_ABI-L2-FDCC-M6_G16_s20232201356171_e20232201358544_c20232201359127.nc']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_blobs = select_blobs(blob_names)\n",
    "selected_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABI-L2-FDCC-M6': <xarray.Dataset>\n",
       " Dimensions:                                           (y: 1500, x: 2500,\n",
       "                                                        number_of_time_bounds: 2,\n",
       "                                                        number_of_image_bounds: 2,\n",
       "                                                        number_of_sunglint_angle_bounds: 2,\n",
       "                                                        number_of_LZA_bounds: 2,\n",
       "                                                        number_of_SZA_bounds: 2)\n",
       " Coordinates:\n",
       "     t                                                 datetime64[ns] ...\n",
       "   * y                                                 (y) float64 0.1282 ... ...\n",
       "   * x                                                 (x) float64 -0.1013 ......\n",
       "     y_image                                           float32 ...\n",
       "     x_image                                           float32 ...\n",
       "     sunglint_angle                                    float32 ...\n",
       "     local_zenith_angle                                float32 ...\n",
       "     solar_zenith_angle                                float32 ...\n",
       " Dimensions without coordinates: number_of_time_bounds, number_of_image_bounds,\n",
       "                                 number_of_sunglint_angle_bounds,\n",
       "                                 number_of_LZA_bounds, number_of_SZA_bounds\n",
       " Data variables: (12/40)\n",
       "     Area                                              (y, x) float32 ...\n",
       "     Temp                                              (y, x) float32 ...\n",
       "     Mask                                              (y, x) float32 ...\n",
       "     Power                                             (y, x) float32 ...\n",
       "     DQF                                               (y, x) float32 ...\n",
       "     time_bounds                                       (number_of_time_bounds) datetime64[ns] ...\n",
       "     ...                                                ...\n",
       "     standard_deviation_fire_radiative_power           float32 ...\n",
       "     algorithm_dynamic_input_data_container            int32 ...\n",
       "     processing_parm_version_container                 int32 ...\n",
       "     algorithm_product_version_container               int32 ...\n",
       "     percent_uncorrectable_GRB_errors                  float32 ...\n",
       "     percent_uncorrectable_L0_errors                   float32 ...\n",
       " Attributes: (12/29)\n",
       "     naming_authority:          gov.nesdis.noaa\n",
       "     Conventions:               CF-1.7\n",
       "     Metadata_Conventions:      Unidata Dataset Discovery v1.0\n",
       "     standard_name_vocabulary:  CF Standard Name Table (v35, 20 July 2016)\n",
       "     institution:               DOC/NOAA/NESDIS > U.S. Department of Commerce,...\n",
       "     project:                   GOES\n",
       "     ...                        ...\n",
       "     cdm_data_type:             Image\n",
       "     time_coverage_start:       2023-08-08T13:56:17.1Z\n",
       "     time_coverage_end:         2023-08-08T13:58:54.4Z\n",
       "     timeline_id:               ABI Mode 6\n",
       "     production_data_source:    Realtime\n",
       "     id:                        c44767ac-160c-4ad7-851c-f3246a8da471}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = get_datasets(selected_blobs)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first dataset from the datasets dictionary\n",
    "first_ds_key = next(iter(datasets))\n",
    "first_ds = datasets[first_ds_key]\n",
    "\n",
    "# Generate GeoJSON points from the first dataset\n",
    "timestamp, geojson_str = generate_geojson_points(first_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Upload the generated GeoJSON to BigQuery\n",
    "upload_to_bigquery(timestamp, geojson_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
