{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function description:\n",
    "\n",
    "1. Select appropriate list of blobs from gcp fs, most recent hour's worth of data from GOES MCMIPC bucket. This should be 12 blobs because there are scans every 5 mins.\n",
    "2. Download the set of blobs, pruning unneeded data such as data quality flag arrays and unused bands, return list of data sets\n",
    "3. Concatenate the 12 datasets into one, effectively creating a dataset with a time dimension\n",
    "4. Take median over the time dimension, so each pixel has median value of the last hour for each band\n",
    "5. Feature engineer the median dataset, adding more informative bands that are ratios of the spectral channels\n",
    "6. Reproject this dataset to epsg 4326\n",
    "7. Download the preprocessed landfire layers. These have been reproject_matched to a GOES CONUS 'template' image, which has itself been reprojected to epsg 4326. This is intended to match the slightly convoluted preprocessing routine of training data the pytorch model was trained on.\n",
    "8. Stack the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "9. Chunk the stacks to pytorch manageable size and upload to a bucket, this will be a large list of dataset files that have the stacked raster imagery with metadata that can be used to project pytorch container's inference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_blobs(bucket_name='gcp-public-data-goes-16'):\n",
    "    \"\"\"\n",
    "    Selects the appropriate list of blobs from GCP fs, most recent hour's worth of data from GOES MCMIPC bucket.\n",
    "    Returns: List of selected blobs.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    attime = datetime.utcnow()\n",
    "\n",
    "    # Set up Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Create a range of directories to check. The GOES bucket is\n",
    "    # organized by hour of day.\n",
    "    selected_blobs = []\n",
    "    for i in range(2):  # Get blobs from current hour and previous hour\n",
    "        current_time = attime - timedelta(hours=i)\n",
    "        prefix = f'ABI-L2-MCMIPC/{current_time.year}/{current_time.timetuple().tm_yday:03d}/{current_time.hour:02d}/'\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        selected_blobs.extend([blob.name for blob in blobs])\n",
    "\n",
    "    # Sort the blobs by their timestamp in descending order\n",
    "    selected_blobs.sort(key=lambda name: name.split('_')[3][1:], reverse=True)\n",
    "\n",
    "    # Check if there are at least 12 blobs\n",
    "    if len(selected_blobs) < 12:\n",
    "        raise Exception(f\"Only {len(selected_blobs)} blobs found\")\n",
    "\n",
    "    return selected_blobs[:12]\n",
    "\n",
    "\n",
    "def create_fs():\n",
    "    \"\"\"\n",
    "    Creates a file system object for GCP. \n",
    "    Returns: File system object. fs can be interacted with as though it were a local file system.\n",
    "    \"\"\"\n",
    "    fs = fsspec.filesystem('gcs', token=os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "    return fs\n",
    "\n",
    "\n",
    "def create_median_image(blob_list, fs, bucket_name='gcp-public-data-goes-16'):\n",
    "    \"\"\"\n",
    "    This function creates a single-band image from a list of blob names.\n",
    "    The pixel values in the image are the median values from the corresponding pixels in the input images.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the Datasets\n",
    "    datasets = []\n",
    "\n",
    "    # Open each blob as a full dataset and load it into memory\n",
    "    for blob in blob_list[::6]:  # Use every 6th blob for testing purposes (faster operation creating each median without pipeline-structure difference), change to 1 later\n",
    "        f = fs.open(f'{bucket_name}/{blob}')\n",
    "        print(f'Opening: {bucket_name}/{blob}')\n",
    "        ds = xr.open_dataset(f).load()\n",
    "\n",
    "        # Select a firey region for testing - NW US basically\n",
    "        ds = ds.isel(x=slice(0, 1000), y=slice(0, 1000))\n",
    "\n",
    "        datasets.append(ds)\n",
    "\n",
    "    # Concatenate the datasets along a new 'band' dimension\n",
    "    concated = xr.concat(datasets, dim='time')\n",
    "\n",
    "    # Compute the median along the 'band' dimension\n",
    "    median_ds = concated.median(dim='time', keep_attrs=True)\n",
    "\n",
    "    # Close the files\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "\n",
    "    # Return the median dataset\n",
    "    return median_ds\n",
    "\n",
    "def download_landfire_layers(fs, bucket_name='firenet_reference', blob_name='combined_landfire.nc'):\n",
    "    \"\"\"\n",
    "    Downloads the preprocessed landfire layers. These have been reproject_matched to a GOES CONUS 'template' image, \n",
    "    which has itself been reprojected to epsg 5070. Properly loading and accessing the spatial metadata uses a\n",
    "    trick, openning with xarray, saving to nc, then opening the nc tempfile with rioxarray.\n",
    "    Returns: Preprocessed landfire layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the blob as a full dataset and load it into memory\n",
    "    f = fs.open(f'{bucket_name}/{blob_name}')\n",
    "    print(f'Opening: {bucket_name}/{blob_name}')\n",
    "    ds = xr.open_dataset(f).load()\n",
    "\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "        # Save the dataset to the temporary file\n",
    "        ds.to_netcdf(tmpfile.name)\n",
    "\n",
    "        # Open the temporary file with rioxarray\n",
    "        landfire_layers = rioxarray.open_rasterio(tmpfile.name)\n",
    "\n",
    "    return landfire_layers\n",
    "\n",
    "def reproject_dataset(dataset, landfire_layers):\n",
    "    \"\"\"\n",
    "    Reprojects the dataset to the static layers.\n",
    "    Note that the technique used here is again creating a tempfile and then opening it with rioxarray.\n",
    "    This is a hack but I could not find any other way to get rioxarray to recognize the spatial\n",
    "    metadata that is necessary for reprojection.\n",
    "    Returns: Reprojected dataset.\n",
    "    \"\"\"\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "        # Save the dataset to the temporary file\n",
    "        dataset.to_netcdf(tmpfile.name)\n",
    "\n",
    "        # Open the temporary file with rioxarray\n",
    "        ds_rio = rioxarray.open_rasterio(tmpfile.name)\n",
    "        \n",
    "        # Reproject the dataset to CRS EPSG:5070 and resample to new resolution\n",
    "        reprojected_dataset = ds_rio.rio.reproject_match(landfire_layers)\n",
    "\n",
    "    return reprojected_dataset\n",
    "\n",
    "def engineer_features(dataset):\n",
    "    \"\"\"\n",
    "    Feature engineers the median dataset, adding more informative bands that are ratios of the spectral channels.\n",
    "    Returns: Feature engineered dataset.\n",
    "    \"\"\"\n",
    "    # Ensure the CRS is preserved by extracting it from the original dataset\n",
    "    original_crs = dataset.rio.crs\n",
    "\n",
    "    # Compute the new features\n",
    "    feat1 = dataset['CMI_C06'] / dataset['CMI_C05']\n",
    "    feat2 = dataset['CMI_C07'] / dataset['CMI_C05']\n",
    "    feat3 = dataset['CMI_C07'] / dataset['CMI_C06']\n",
    "    feat4 = dataset['CMI_C14'] / dataset['CMI_C07']\n",
    "\n",
    "    # Create a dictionary of the new features\n",
    "    data_dict = {'feat_6_5': feat1, 'feat_7_5': feat2, 'feat_7_6': feat3, 'feat_14_7': feat4}\n",
    "\n",
    "    # Add the new features to the dataset\n",
    "    engineered_dataset = dataset.assign(data_dict)\n",
    "\n",
    "    # Write the CRS of original_dataset to engineered_dataset, as a global attribute\n",
    "    engineered_dataset.rio.write_crs(original_crs, inplace=True)\n",
    "\n",
    "    # Write the CRS to every variable in engineered_dataset, making all var attrs match\n",
    "    for var in engineered_dataset.data_vars:\n",
    "        engineered_dataset[var].rio.write_crs(original_crs, inplace=True)\n",
    "\n",
    "    return engineered_dataset\n",
    "\n",
    "def stack_datasets(goes_ds, landfire_layers):\n",
    "    \"\"\"\n",
    "    Stacks the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "    Returns: Stacked dataset.\n",
    "    \"\"\"\n",
    "    # Merge the two datasets\n",
    "    stacked_dataset = xr.merge([goes_ds, landfire_layers])\n",
    "\n",
    "    return stacked_dataset\n",
    "\n",
    "\n",
    "def chunk_image(multiband_image, chunk_size=64):\n",
    "    \"\"\"\n",
    "    Splits the multiband image into chunks.\n",
    "    Args:\n",
    "        multiband_image (xarray.DataArray): The multiband image to be chunked.\n",
    "        chunk_size (int): The size of the chunks. Default is 64.\n",
    "    Returns:\n",
    "        chunks (list): A list of xarray Datasets representing the chunks.\n",
    "        spatial_info (list): A list of tuples representing the spatial information of each chunk.\n",
    "    \"\"\"\n",
    "    # Get the width and height of the image\n",
    "    width = multiband_image.dims['x']\n",
    "    height = multiband_image.dims['y']\n",
    "\n",
    "    # Calculate the number of chunks in x and y direction\n",
    "    nx, ny = width // chunk_size, height // chunk_size\n",
    "\n",
    "    # Initialize a list to store the chunks\n",
    "    chunks = []\n",
    "    spatial_info = []\n",
    "\n",
    "    # Loop over the image\n",
    "    for i in range(ny):\n",
    "        for j in range(nx):\n",
    "            # Define the slice\n",
    "            y_slice = slice(i * chunk_size, (i + 1) * chunk_size)\n",
    "            x_slice = slice(j * chunk_size, (j + 1) * chunk_size)\n",
    "\n",
    "            # Extract the chunk across all bands\n",
    "            chunk = multiband_image.isel(y=y_slice, x=x_slice)\n",
    "\n",
    "            # Store the chunk and its spatial information\n",
    "            chunks.append(chunk)\n",
    "            spatial_info.append((y_slice, x_slice))\n",
    "\n",
    "    return chunks, spatial_info\n",
    "\n",
    "def save_chunks_to_bucket(fs, chunks, spatial_info, bucket_name):\n",
    "    \"\"\"\n",
    "    Saves the chunks to a bucket.\n",
    "    Args:\n",
    "        chunks (list): A list of xarray Datasets representing the chunks.\n",
    "        spatial_info (list): A list of tuples representing the spatial information of each chunk.\n",
    "        bucket_name (str): The name of the bucket.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop over the chunks and their corresponding spatial information\n",
    "    for i, chunk in enumerate(chunks):\n",
    "\n",
    "        # Add the spatial information as an attribute to the chunk\n",
    "        chunk.attrs['spatial_info'] = str(spatial_info[i])\n",
    "\n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "            # Save the chunk to the temporary file\n",
    "            chunk.to_netcdf(tmpfile.name)\n",
    "\n",
    "            # Upload the temporary file to the bucket\n",
    "            fs.put(tmpfile.name, f'{bucket_name}/chunk_{i}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_blobs = select_blobs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/030/05/OR_ABI-L2-MCMIPC-M6_G16_s20240300556173_e20240300558552_c20240300559069.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/030/05/OR_ABI-L2-MCMIPC-M6_G16_s20240300526173_e20240300528546_c20240300529060.nc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fs = create_fs()\n",
    "median_ds = create_median_image(selected_blobs, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: firenet_reference/combined_landfire.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_38049/500997185.py:107: SerializationWarning: saving variable y with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n",
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_38049/500997185.py:107: SerializationWarning: saving variable x with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n"
     ]
    }
   ],
   "source": [
    "landfire_layers = download_landfire_layers(fs)\n",
    "reprojected_median_ds = reproject_dataset(median_ds, landfire_layers)\n",
    "reprojected_median_ds = engineer_features(reprojected_median_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ds = stack_datasets(reprojected_median_ds, landfire_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, spatial_info = chunk_image(stacked_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_chunks_to_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpreprocessed_firenet_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 217\u001b[0m, in \u001b[0;36msave_chunks_to_bucket\u001b[0;34m(fs, chunks, spatial_info, bucket_name)\u001b[0m\n\u001b[1;32m    214\u001b[0m chunk\u001b[38;5;241m.\u001b[39mto_netcdf(tmpfile\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Upload the temporary file to the bucket\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmpfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/chunk_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/streamlitenv/lib/python3.11/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/streamlitenv/lib/python3.11/site-packages/fsspec/asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/streamlitenv/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/streamlitenv/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_chunks_to_bucket(fs, chunks, spatial_info, 'preprocessed_firenet_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks[:100]):\n",
    "    chunk.to_netcdf(f'/Users/adamhunter/Documents/school projs/firenet/data/testing_inputs/chunk_{i}.nc')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
