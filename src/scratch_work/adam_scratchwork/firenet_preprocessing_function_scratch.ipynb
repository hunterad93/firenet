{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function description:\n",
    "\n",
    "1. Select appropriate list of blobs from gcp fs, most recent hour's worth of data from GOES MCMIPC bucket. This should be 12 blobs because there are scans every 5 mins.\n",
    "2. Download the set of blobs, pruning unneeded data such as data quality flag arrays and unused bands, return list of data sets\n",
    "3. Concatenate the 12 datasets into one, effectively creating a dataset with a time dimension\n",
    "4. Take median over the time dimension, so each pixel has median value of the last hour for each band\n",
    "5. Feature engineer the median dataset, adding more informative bands that are ratios of the spectral channels\n",
    "6. Download the preprocessed landfire layers. Preproprecessing done in `reprojectmatch_and_stack_landfire_for_bucket.ipynb`. Reproject_match GOES to this layers\n",
    "7. Stack the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "8. Pull data vars out as a 21 dimensional numpy array, leaving a 'template raster' which has nan data values and all the spatial metadata attached.\n",
    "9. Chunk the numpy array into 21x64x64 chunks, normalize each chunk per axis 0 level. In otherwords normalize each band to values between 0 and 1.\n",
    "10. Pass fully processed chunks through the unet model, receiving 1x64x64 numpy arrays on other side.\n",
    "11. Stitch the predicted chunks back together, then replace the template raster empty array with the stitched predictions.\n",
    "12. Now we have an nc with values between 0 and 1, 0 reflecting no fire 1 reflecting yes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import fsspec\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from xarray.backends import NetCDF4DataStore\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing functions, this set of functions accomplishes downloading and matching input data, and separating the data arrays from the spatial metadata. Function for stitching the UNET prediction arrays also in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_blobs(bucket_name='gcp-public-data-goes-16'):\n",
    "    \"\"\"\n",
    "    Selects the appropriate list of blobs from GCP fs, most recent hour's worth of data from GOES MCMIPC bucket.\n",
    "    Returns: List of selected blobs.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    attime = datetime.now()\n",
    "    attime = datetime(2020, 8, 20, 17, 0)\n",
    "\n",
    "    # Set up Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Create a range of directories to check. The GOES bucket is\n",
    "    # organized by hour of day.\n",
    "    selected_blobs = []\n",
    "    for i in range(2):  # Get blobs from current hour and previous hour\n",
    "        current_time = attime - timedelta(hours=i)\n",
    "        prefix = f'ABI-L2-MCMIPC/{current_time.year}/{current_time.timetuple().tm_yday:03d}/{current_time.hour:02d}/'\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        selected_blobs.extend([blob.name for blob in blobs])\n",
    "\n",
    "    # Sort the blobs by their timestamp in descending order\n",
    "    selected_blobs.sort(key=lambda name: name.split('_')[3][1:], reverse=True)\n",
    "\n",
    "    # Check if there are at least 12 blobs\n",
    "    if len(selected_blobs) < 12:\n",
    "        raise Exception(f\"Only {len(selected_blobs)} blobs found\")\n",
    "\n",
    "    return selected_blobs[:12]\n",
    "\n",
    "\n",
    "def create_fs():\n",
    "    \"\"\"\n",
    "    Creates a file system object for GCP. \n",
    "    Returns: File system object. fs can be interacted with as though it were a local file system.\n",
    "    \"\"\"\n",
    "    fs = fsspec.filesystem('gcs', token=os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "    return fs\n",
    "\n",
    "\n",
    "def download_blob(fs, bucket_name, blob):\n",
    "    \"\"\"\n",
    "    Download a single blob and load it into an xarray Dataset.\n",
    "    \"\"\"\n",
    "    with fs.open(f'{bucket_name}/{blob}') as f:\n",
    "        ds = xr.open_dataset(f).load()\n",
    "        # Apply any necessary preprocessing here\n",
    "    return ds\n",
    "\n",
    "def create_median_image_parallel(blob_list, fs, bucket_name='gcp-public-data-goes-16'):\n",
    "    datasets = []\n",
    "    # Use ThreadPoolExecutor to parallelize the download and loading of datasets\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_blob = {executor.submit(download_blob, fs, bucket_name, blob): blob for blob in blob_list[8::]}\n",
    "        for future in concurrent.futures.as_completed(future_to_blob):\n",
    "            blob = future_to_blob[future]\n",
    "            try:\n",
    "                ds = future.result()\n",
    "                # Apply any region selection or additional preprocessing here if needed\n",
    "                datasets.append(ds)\n",
    "            except Exception as exc:\n",
    "                print(f'{blob} generated an exception: {exc}')\n",
    "    # Continue with concatenation and median calculation\n",
    "    concated = xr.concat(datasets, dim='time')\n",
    "    median_ds = concated.median(dim='time', keep_attrs=True)\n",
    "    return median_ds\n",
    "\n",
    "def download_landfire_layers(fs, bucket_name='firenet_reference', blob_name='combined_landfire.nc'):\n",
    "    \"\"\"\n",
    "    Downloads the preprocessed landfire layers. These have been reproject_matched to a GOES CONUS 'template' image, \n",
    "    which has itself been reprojected to epsg 5070. Properly loading and accessing the spatial metadata uses a\n",
    "    trick, openning with xarray, saving to nc, then opening the nc tempfile with rioxarray. This is not a \"good\"\n",
    "    approach but for whatever reason the spatial metadata couldn't be accessed otherwise. Trying to open directly with\n",
    "    rioxarray runs into some interfacing problem with google buckets.\n",
    "    Returns: Preprocessed landfire layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the blob as a full dataset and load it into memory\n",
    "    f = fs.open(f'{bucket_name}/{blob_name}')\n",
    "    print(f'Opening: {bucket_name}/{blob_name}')\n",
    "    ds = xr.open_dataset(f).load()\n",
    "\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "        # Save the dataset to the temporary file\n",
    "        ds.to_netcdf(tmpfile.name)\n",
    "\n",
    "        # Open the temporary file with rioxarray\n",
    "        landfire_layers = rioxarray.open_rasterio(tmpfile.name)\n",
    "\n",
    "    return landfire_layers\n",
    "\n",
    "def reproject_dataset(dataset, landfire_layers):\n",
    "    \"\"\"\n",
    "    Reprojects the dataset to the static layers.\n",
    "    Note that the technique used here is again creating a tempfile and then opening it with rioxarray.\n",
    "    This is not a \"good\" approach but for whatever reason the spatial metadata couldn't be accessed otherwise.\n",
    "    Trying to open directly with rioxarray runs into some interfacing problem with google buckets.\n",
    "    Returns: Reprojected dataset.\n",
    "    \"\"\"\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "        # Save the dataset to the temporary file\n",
    "        dataset.to_netcdf(tmpfile.name)\n",
    "\n",
    "        # Open the temporary file with rioxarray\n",
    "        ds_rio = rioxarray.open_rasterio(tmpfile.name)\n",
    "        \n",
    "        # Reproject the dataset to the template dataset with landfire layers, landfire layers was generated by\n",
    "        #  `reprojectmatch_and_stack_landfire_for_bucket.ipynb`\n",
    "        reprojected_dataset = ds_rio.rio.reproject_match(landfire_layers)\n",
    "\n",
    "    return reprojected_dataset\n",
    "\n",
    "def engineer_features(dataset):\n",
    "    \"\"\"\n",
    "    Feature engineers the median dataset, adding more informative bands that are ratios of the spectral channels.\n",
    "    Returns: Feature engineered dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    vars_to_remove = [f for f in dataset.keys() if f.startswith('DQF_')]\n",
    "    dataset = dataset.drop_vars(vars_to_remove)\n",
    "    # Ensure the CRS is preserved by extracting it from the original dataset\n",
    "    original_crs = dataset.rio.crs\n",
    "\n",
    "    # Compute the new features\n",
    "    feat1 = dataset['CMI_C06'] / dataset['CMI_C05']\n",
    "    feat2 = dataset['CMI_C07'] / dataset['CMI_C05']\n",
    "    feat3 = dataset['CMI_C07'] / dataset['CMI_C06']\n",
    "    feat4 = dataset['CMI_C14'] / dataset['CMI_C07']\n",
    "\n",
    "    # Create a dictionary of the new features\n",
    "    data_dict = {'feat_6_5': feat1, 'feat_7_5': feat2, 'feat_7_6': feat3, 'feat_14_7': feat4}\n",
    "\n",
    "    # Add the new features to the dataset\n",
    "    engineered_dataset = dataset.assign(data_dict)\n",
    "\n",
    "    # Write the CRS of original_dataset to engineered_dataset, as a global attribute\n",
    "    engineered_dataset.rio.write_crs(original_crs, inplace=True)\n",
    "\n",
    "    # Write the CRS to every variable in engineered_dataset, making all var attrs match\n",
    "    for var in engineered_dataset.data_vars:\n",
    "        engineered_dataset[var].rio.write_crs(original_crs, inplace=True)\n",
    "\n",
    "    return engineered_dataset\n",
    "\n",
    "def stack_datasets(goes_ds, landfire_layers):\n",
    "    \"\"\"\n",
    "    Stacks the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "    Sets 'grid_mapping' to 'spatial_ref' in the encoding for every data variable in the process.\n",
    "    Returns: Stacked dataset.\n",
    "    \"\"\"\n",
    "    # Merge the two datasets\n",
    "    stacked_dataset = xr.merge([goes_ds, landfire_layers])\n",
    "\n",
    "    # Set 'grid_mapping' to 'spatial_ref' in the encoding dictionary for every data variable\n",
    "    # Pointing this encoding key to the global spatial_ref value is vital to ensure spatial metadata\n",
    "    # Gets recognized by future rioxarray and other operations\n",
    "    for var in stacked_dataset.data_vars:\n",
    "        stacked_dataset[var].encoding['grid_mapping'] = 'spatial_ref'\n",
    "\n",
    "    # Optionally, delete 'goes_imager_projection' if it's no longer needed\n",
    "    if 'goes_imager_projection' in stacked_dataset:\n",
    "        del stacked_dataset['goes_imager_projection']\n",
    "\n",
    "    return stacked_dataset\n",
    "\n",
    "def prune_dataset(ds):\n",
    "    \"\"\"\n",
    "    Drops specified data variables from the dataset.\n",
    "    #TODO make sure the indices to drop match perfectly the original training\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray.Dataset to be pruned.\n",
    "    - vars_to_drop: List of strings representing the names of the variables to drop.\n",
    "\n",
    "    Returns:\n",
    "    - ds: xarray.Dataset after dropping the specified variables.\n",
    "    \"\"\"\n",
    "    # Drop the specified variables from the dataset\n",
    "    var_names_to_drop = [list(ds.data_vars)[i] for i in [0, 11, 16, 17, 18]]\n",
    "\n",
    "    ds = ds.drop_vars(var_names_to_drop)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def create_spatial_template(dataset):\n",
    "    \"\"\"\n",
    "    Creates a spatial template from the original dataset by keeping only one data variable\n",
    "    and setting its values to 0, while preserving spatial metadata. This step allows us to\n",
    "    Take the datavars out as a 21x64x64 numpy array, then add the model output back as 1x64x64\n",
    "    so that the spatial projection of the output is untouched.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: xarray.Dataset or rioxarray object with spatial dimensions and CRS information.\n",
    "    \n",
    "    Returns:\n",
    "    - template: xarray.Dataset with a single data variable filled with NaNs and original spatial metadata.\n",
    "    \"\"\"\n",
    "    # Clone the dataset to avoid modifying the original\n",
    "    template = dataset.copy()\n",
    "    \n",
    "    # Select the first data variable (assuming there's at least one)\n",
    "    first_var_name = list(template.data_vars)[0]\n",
    "    first_var = template[first_var_name]\n",
    "    \n",
    "    # Create a 0-filled template of the first variable\n",
    "    nan_template = xr.full_like(first_var, fill_value=np.nan)\n",
    "    \n",
    "    # Remove all data variables from the template\n",
    "    for var_name in list(template.data_vars):\n",
    "        del template[var_name]\n",
    "    \n",
    "    # Add the NaN-filled template variable back\n",
    "    template[first_var_name] = nan_template\n",
    "    \n",
    "    # Ensure the spatial metadata is preserved\n",
    "    # Note: This step might be redundant if the metadata is already attached to the coordinates\n",
    "    # and not the data variables themselves. However, it's a safeguard for maintaining CRS.\n",
    "    if hasattr(dataset, 'rio') and hasattr(dataset.rio, 'crs'):\n",
    "        template.rio.write_crs(dataset.rio.crs, inplace=True)\n",
    "    \n",
    "    return template\n",
    "\n",
    "def extract_data_as_array(dataset):\n",
    "    \"\"\"\n",
    "    This function pulls the data variable value arrays out of the xarray dataset.\n",
    "    \"\"\"\n",
    "    # Stack the data variables, then use np.squeeze() to remove the singleton dimension that was a placeholder for time.\n",
    "    # The resultant array should be the shape 42, 3506, 2266. The second two dimensions may change if region of interest changes.\n",
    "\n",
    "    stacked_array = np.stack([dataset[var].values for var in dataset.data_vars], axis=0)\n",
    "    squeezed_array = np.squeeze(stacked_array)\n",
    "    return squeezed_array\n",
    "\n",
    "def chunk_ndarray(arr, chunk_size=64):\n",
    "    \"\"\"\n",
    "    Breaks down an N-dimensional array into chunks along the last two dimensions,\n",
    "    keeping the first dimension intact in each chunk.\n",
    "    \n",
    "    Parameters:\n",
    "    - arr: Input N-dimensional NumPy array with shape (Variables, Height, Width).\n",
    "    - chunk_size: Size of the chunks along each of the last two dimensions.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of chunks, where each chunk is an N-dimensional NumPy array with shape (Variables, chunk_size, chunk_size).\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    # Iterate over the last two dimensions in steps of `chunk_size`\n",
    "    for i in range(0, arr.shape[1], chunk_size):  # Height dimension\n",
    "        for j in range(0, arr.shape[2], chunk_size):  # Width dimension\n",
    "            # Calculate the end indices while ensuring they do not exceed the array's dimensions\n",
    "            end_i = min(i + chunk_size, arr.shape[1])\n",
    "            end_j = min(j + chunk_size, arr.shape[2])\n",
    "            # Extract the chunk\n",
    "            chunk = arr[:, i:end_i, j:end_j]\n",
    "            # Only add chunks that meet the full size requirement (i.e., 42x64x64)\n",
    "            if chunk.shape[1] == chunk_size and chunk.shape[2] == chunk_size:\n",
    "                chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_chunks(chunks):\n",
    "    # Extract the first band from each chunk\n",
    "    return [chunk[0] for chunk in chunks]\n",
    "\n",
    "def stitch_chunks(processed_chunks, original_shape):\n",
    "    # Assuming processed_chunks is a list of 2D arrays (Height, Width)\n",
    "    # and original_shape is the shape of the 2D plane of the original array (Height, Width)\n",
    "    stitched = np.zeros(original_shape)\n",
    "    chunk_size = processed_chunks[0].shape[0]\n",
    "    for k, chunk in enumerate(processed_chunks):\n",
    "        i, j = divmod(k, original_shape[1] // chunk_size)\n",
    "        stitched[i*chunk_size:(i+1)*chunk_size, j*chunk_size:(j+1)*chunk_size] = chunk\n",
    "    return stitched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_blobs = select_blobs()\n",
    "fs = create_fs()\n",
    "median_ds = create_median_image_parallel(selected_blobs, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landfire_layers = download_landfire_layers(fs)\n",
    "reprojected_median_ds = reproject_dataset(median_ds, landfire_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reprojected_median_ds = engineer_features(reprojected_median_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ds = stack_datasets(reprojected_median_ds, landfire_layers)\n",
    "stacked_ds = prune_dataset(stacked_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ds.to_netcdf(\"stacked_dataset.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = create_spatial_template(stacked_ds)\n",
    "npy_array = extract_data_as_array(stacked_ds)\n",
    "npy_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks = chunk_ndarray(npy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_chunks = process_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched = stitch_chunks(processed_chunks, npy_array.shape[-2:])\n",
    "template[list(stacked_ds.data_vars)[0]].values = np.expand_dims(stitched, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template.to_netcdf(\"template.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import fsspec\n",
    "from xarray.backends import NetCDF4DataStore\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(InConv, self).__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels, classes):\n",
    "        super(Unet, self).__init__()\n",
    "        self.n_channels = in_channels\n",
    "        self.n_classes =  classes\n",
    "\n",
    "        self.inc = InConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fire_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, mode=\"train\", transform=None):\n",
    "\n",
    "        assert mode in {\"train\", \"valid\", \"test\"}\n",
    "\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images_directory = os.path.join(self.root, \"images\")\n",
    "        self.masks_directory = os.path.join(self.root, \"annotations\")\n",
    " \n",
    "        self.filenames = self._get_files(suffix = '.npy')  # read train/valid/test splits\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        filename = self.filenames[idx]\n",
    "        \n",
    "        image_path = os.path.join(self.images_directory, filename)\n",
    "        mask_path = os.path.join(self.masks_directory, filename)\n",
    "        \n",
    "        image = torch.from_numpy(np.load(image_path))\n",
    "        mask = torch.from_numpy(np.load(mask_path))\n",
    "        \n",
    "        sample = dict(image = image, mask = mask)\n",
    "        \n",
    "        \n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def _get_files(self, suffix):\n",
    "        files = []\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            dir_to_walk = os.path.join(self.root, 'images')\n",
    "        elif self.mode == 'valid':\n",
    "            dir_to_walk = os.path.join(self.root, 'annotations')\n",
    "            \n",
    "            \n",
    "        for r,d,f in os.walk(dir_to_walk):\n",
    "            for file in f:\n",
    "                if file.endswith(suffix):\n",
    "                    files.append(os.path.join(r,file))\n",
    "        \n",
    "        \n",
    "        if self.mode == \"train\":  # 90% for train\n",
    "            filenames = [x for i, x in enumerate(files) if i % 10 != 0]\n",
    "        elif self.mode == \"valid\":  # 10% for validation\n",
    "            filenames = [x for i, x in enumerate(files) if i % 10 == 0]\n",
    "\n",
    "        filenames = [f.split('/')[-1] for f in filenames]\n",
    "        return filenames\n",
    "    \n",
    "    def return_files(self):\n",
    "        return self.filenames\n",
    "    \n",
    "    def _normalize(tensor):\n",
    "        mean, std, var = torch.mean(tensor), torch.std(tensor), torch.var(tensor)\n",
    "\n",
    "        return (tensor-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_metric(inputs, target):\n",
    "    intersection = 2.0 * (target * inputs).sum()\n",
    "    union = target.sum() + inputs.sum()\n",
    "    if target.sum() == 0 and inputs.sum() == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "def dice_loss(inputs, target):\n",
    "    num = target.size(0)\n",
    "    inputs = inputs.reshape(num, -1)\n",
    "    target = target.reshape(num, -1)\n",
    "    smooth = 1.0\n",
    "    intersection = (inputs * target)\n",
    "    dice = (2. * intersection.sum(1) + smooth) / (inputs.sum(1) + target.sum(1) + smooth)\n",
    "    dice = 1 - dice.sum() / num\n",
    "    return dice\n",
    "\n",
    "def bce_dice_loss(inputs, target):\n",
    "    dicescore = dice_loss(inputs, target)\n",
    "    bcescore = nn.BCELoss()\n",
    "    bceloss = bcescore(inputs, target)\n",
    "\n",
    "    return bceloss + dicescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "unet = Unet(21, 1)\n",
    "unet.load_state_dict(torch.load('/Users/adamhunter/Documents/school projs/firenet/data/model2.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bands(np_arr):\n",
    "    \"\"\"\n",
    "    Normalizes each band in a 3D array independently.\n",
    "    \n",
    "    Parameters:\n",
    "    - np_arr: A 3D NumPy array of shape (bands, height, width).\n",
    "    \n",
    "    Returns:\n",
    "    - A 3D NumPy array of the same shape, with each band normalized.\n",
    "    \"\"\"\n",
    "    normalized = np.zeros_like(np_arr, dtype=np.float32)\n",
    "    for i in range(np_arr.shape[0]):\n",
    "        band = np_arr[i]\n",
    "        mini, maxi = np.min(band), np.max(band)\n",
    "        if maxi != mini:\n",
    "            normalized[i] = (band - mini) / (maxi - mini)\n",
    "        else:\n",
    "            normalized[i] = band  #TODO should a band that gets here be turned into nan instead?\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_chunks = []\n",
    "for chunk in chunks:\n",
    "    normalized_chunk = normalize_bands(chunk)\n",
    "    normalized_chunks.append(normalized_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show histograms of the first 10 chunk values before normalization\n",
    "for i, chunk in enumerate(chunks[100:110]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Flatten the chunk to make it suitable for histogram plotting\n",
    "    flattened_chunk = chunk.flatten()\n",
    "    plt.hist(flattened_chunk, bins=50, color='red', edgecolor='black')\n",
    "    plt.title(f'Histogram of Chunk {i+1} Values Before Normalization')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show histograms of the first 10 normalized chunk values\n",
    "for i, chunk in enumerate(normalized_chunks[1100:1110]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Flatten the chunk to make it suitable for histogram plotting\n",
    "    flattened_chunk = chunk.flatten()\n",
    "    plt.hist(flattened_chunk, bins=50, color='blue', edgecolor='black')\n",
    "    plt.title(f'Histogram of Normalized Chunk {i+1}')\n",
    "    plt.xlabel('Normalized Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "# Assuming `chunks` is a list of your 3D arrays (21x64x64)\n",
    "for chunk in chunks:\n",
    "    # Normalize the current chunk\n",
    "    normalized_chunk = normalize_bands(chunk)\n",
    "    \n",
    "    # Convert the normalized chunk to a PyTorch tensor\n",
    "    input_data = torch.from_numpy(normalized_chunk).float()\n",
    "    \n",
    "    # Add a batch dimension, just to fit with the model this is a singleton\n",
    "    input_data = input_data.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    # Now, input_data is ready to be fed into the U-Net model\n",
    "    # Append the prediction for the current chunk to a list of predictions\n",
    "    predictions.append(unet(input_data))\n",
    "    \n",
    "    # Process the predictions as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a grid of the first 10 predictions arrays\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 8))\n",
    "for i in range(10):\n",
    "    ax = axes[i//5, i%5]\n",
    "    prediction_array = predictions[i].detach().numpy().squeeze()\n",
    "    im = ax.imshow(prediction_array, cmap='viridis')\n",
    "    ax.set_title(f'Prediction {i+1}')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_np_arrays = [p.detach().numpy().squeeze() for p in predictions]\n",
    "stitched = stitch_chunks(stitched_np_arrays, npy_array.shape[-2:])\n",
    "stitched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the locations of non-zero values in the stitched image\n",
    "plt.figure(figsize=(10, 6))\n",
    "binary_stitched = np.where(stitched != 0, 1, 0)  # Convert stitched array to binary\n",
    "plt.imshow(binary_stitched, cmap='gray', interpolation='none')  # Display as binary image\n",
    "plt.title('Binary Representation of Stitched Image')\n",
    "plt.xlabel('Pixel X Coordinate')\n",
    "plt.ylabel('Pixel Y Coordinate')\n",
    "plt.show()\n",
    "\n",
    "# Count of 0 and non-zero values in the stitched image\n",
    "zero_vals = np.sum(stitched == 0)\n",
    "non_zero_vals = np.sum(stitched != 0)\n",
    "\n",
    "# Plot a histogram of the distribution of non-zero values in the stitched image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(stitched[stitched != 0].flatten(), bins=50, color='blue', edgecolor='black')\n",
    "plt.title('Histogram of Non-Zero Values in Stitched Image')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"Count of 0 values: {zero_vals}\")\n",
    "print(f\"Count of non-zero values: {non_zero_vals}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template[list(stacked_ds.data_vars)[0]].values = np.expand_dims(stitched, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified template as a .nc file\n",
    "output_filename = \"/Users/adamhunter/Documents/school projs/firenet/data/processed_data2.nc\"\n",
    "template.to_netcdf(output_filename)\n",
    "print(f\"Data saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
