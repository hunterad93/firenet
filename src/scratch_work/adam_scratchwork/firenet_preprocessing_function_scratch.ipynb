{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function description:\n",
    "\n",
    "1. Select appropriate list of blobs from gcp fs, most recent hour's worth of data from GOES MCMIPC bucket. This should be 12 blobs because there are scans every 5 mins.\n",
    "2. Download the set of blobs, pruning unneeded data such as data quality flag arrays and unused bands, return list of data sets\n",
    "3. Concatenate the 12 datasets into one, effectively creating a dataset with a time dimension\n",
    "4. Take median over the time dimension, so each pixel has median value of the last hour for each band\n",
    "5. Feature engineer the median dataset, adding more informative bands that are ratios of the spectral channels\n",
    "6. Reproject this dataset to epsg 4326\n",
    "7. Download the preprocessed landfire layers. These have been reproject_matched to a GOES CONUS 'template' image, which has itself been reprojected to epsg 4326. This is intended to match the slightly convoluted preprocessing routine of training data the pytorch model was trained on.\n",
    "8. Stack the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "9. Chunk the stacks to pytorch manageable size and upload to a bucket, this will be a large list of dataset files that have the stacked raster imagery with metadata that can be used to project pytorch container's inference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import fsspec\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_blobs(bucket_name='gcp-public-data-goes-16'):\n",
    "    \"\"\"\n",
    "    Selects the appropriate list of blobs from GCP fs, most recent hour's worth of data from GOES MCMIPC bucket.\n",
    "    Returns: List of selected blobs.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    attime = datetime.utcnow()\n",
    "\n",
    "    # Set up Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Create a range of directories to check. The GOES bucket is\n",
    "    # organized by hour of day.\n",
    "    selected_blobs = []\n",
    "    for i in range(2):  # Get blobs from current hour and previous hour\n",
    "        current_time = attime - timedelta(hours=i)\n",
    "        prefix = f'ABI-L2-MCMIPC/{current_time.year}/{current_time.timetuple().tm_yday:03d}/{current_time.hour:02d}/'\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        selected_blobs.extend([blob.name for blob in blobs])\n",
    "\n",
    "    # Sort the blobs by their timestamp in descending order\n",
    "    selected_blobs.sort(key=lambda name: name.split('_')[3][1:], reverse=True)\n",
    "\n",
    "    # Check if there are at least 12 blobs\n",
    "    if len(selected_blobs) < 12:\n",
    "        raise Exception(f\"Only {len(selected_blobs)} blobs found\")\n",
    "\n",
    "    return selected_blobs[:12]\n",
    "\n",
    "\n",
    "def create_fs():\n",
    "    \"\"\"\n",
    "    Creates a file system object for GCP. \n",
    "    Returns: File system object. fs can be interacted with as though it were a local file system.\n",
    "    \"\"\"\n",
    "    fs = fsspec.filesystem('gcs', token=os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "    return fs\n",
    "\n",
    "\n",
    "def create_median_image(blob_list, fs, bucket_name='gcp-public-data-goes-16'):\n",
    "    \"\"\"\n",
    "    This function creates a single-band image from a list of blob names.\n",
    "    The pixel values in the image are the median values from the corresponding pixels in the input images.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the Datasets\n",
    "    datasets = []\n",
    "\n",
    "    # Open each blob as a full dataset and load it into memory\n",
    "    for blob in blob_list[4::]:  # Use every 6th blob for testing purposes (faster operation creating each median without pipeline-structure difference), change to 1 later\n",
    "        f = fs.open(f'{bucket_name}/{blob}')\n",
    "        print(f'Opening: {bucket_name}/{blob}')\n",
    "        ds = xr.open_dataset(f).load()\n",
    "\n",
    "        # Select a firey region for testing - large NW US region\n",
    "        ds = ds.isel(x=slice(0, 1250), y=slice(0, 1250))\n",
    "\n",
    "        datasets.append(ds)\n",
    "\n",
    "    # Concatenate the datasets along a new 'band' dimension\n",
    "    concated = xr.concat(datasets, dim='time')\n",
    "\n",
    "    # Compute the median along the 'band' dimension\n",
    "    median_ds = concated.median(dim='time', keep_attrs=True)\n",
    "\n",
    "    # Close the files\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "\n",
    "    # Return the median dataset\n",
    "    return median_ds\n",
    "\n",
    "def download_landfire_layers(fs, bucket_name='firenet_reference', blob_name='combined_landfire.nc'):\n",
    "    \"\"\"\n",
    "    Downloads the preprocessed landfire layers. These have been reproject_matched to a GOES CONUS 'template' image, \n",
    "    which has itself been reprojected to epsg 5070. Properly loading and accessing the spatial metadata uses a\n",
    "    trick, openning with xarray, saving to nc, then opening the nc tempfile with rioxarray. This is not a \"good\"\n",
    "    approach but for whatever reason the spatial metadata couldn't be accessed otherwise. Trying to open directly with\n",
    "    rioxarray runs into some interfacing problem with google buckets.\n",
    "    Returns: Preprocessed landfire layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the blob as a full dataset and load it into memory\n",
    "    f = fs.open(f'{bucket_name}/{blob_name}')\n",
    "    print(f'Opening: {bucket_name}/{blob_name}')\n",
    "    ds = xr.open_dataset(f).load()\n",
    "\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "        # Save the dataset to the temporary file\n",
    "        ds.to_netcdf(tmpfile.name)\n",
    "\n",
    "        # Open the temporary file with rioxarray\n",
    "        landfire_layers = rioxarray.open_rasterio(tmpfile.name)\n",
    "\n",
    "    return landfire_layers\n",
    "\n",
    "def reproject_dataset(dataset, landfire_layers):\n",
    "    \"\"\"\n",
    "    Reprojects the dataset to the static layers.\n",
    "    Note that the technique used here is again creating a tempfile and then opening it with rioxarray.\n",
    "    This is not a \"good\" approach but for whatever reason the spatial metadata couldn't be accessed otherwise.\n",
    "    Trying to open directly with rioxarray runs into some interfacing problem with google buckets.\n",
    "    Returns: Reprojected dataset.\n",
    "    \"\"\"\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nc') as tmpfile:\n",
    "        # Save the dataset to the temporary file\n",
    "        dataset.to_netcdf(tmpfile.name)\n",
    "\n",
    "        # Open the temporary file with rioxarray\n",
    "        ds_rio = rioxarray.open_rasterio(tmpfile.name)\n",
    "        \n",
    "        # Reproject the dataset to the template dataset with landfire layers, landfire layers was generated by\n",
    "        #  `reprojectmatch_and_stack_landfire_for_bucket.ipynb`\n",
    "        reprojected_dataset = ds_rio.rio.reproject_match(landfire_layers)\n",
    "\n",
    "    return reprojected_dataset\n",
    "\n",
    "def engineer_features(dataset):\n",
    "    \"\"\"\n",
    "    Feature engineers the median dataset, adding more informative bands that are ratios of the spectral channels.\n",
    "    Returns: Feature engineered dataset.\n",
    "    \"\"\"\n",
    "    # Ensure the CRS is preserved by extracting it from the original dataset\n",
    "    original_crs = dataset.rio.crs\n",
    "\n",
    "    # Compute the new features\n",
    "    feat1 = dataset['CMI_C06'] / dataset['CMI_C05']\n",
    "    feat2 = dataset['CMI_C07'] / dataset['CMI_C05']\n",
    "    feat3 = dataset['CMI_C07'] / dataset['CMI_C06']\n",
    "    feat4 = dataset['CMI_C14'] / dataset['CMI_C07']\n",
    "\n",
    "    # Create a dictionary of the new features\n",
    "    data_dict = {'feat_6_5': feat1, 'feat_7_5': feat2, 'feat_7_6': feat3, 'feat_14_7': feat4}\n",
    "\n",
    "    # Add the new features to the dataset\n",
    "    engineered_dataset = dataset.assign(data_dict)\n",
    "\n",
    "    # Write the CRS of original_dataset to engineered_dataset, as a global attribute\n",
    "    engineered_dataset.rio.write_crs(original_crs, inplace=True)\n",
    "\n",
    "    # Write the CRS to every variable in engineered_dataset, making all var attrs match\n",
    "    for var in engineered_dataset.data_vars:\n",
    "        engineered_dataset[var].rio.write_crs(original_crs, inplace=True)\n",
    "\n",
    "    return engineered_dataset\n",
    "\n",
    "def stack_datasets(goes_ds, landfire_layers):\n",
    "    \"\"\"\n",
    "    Stacks the GOES ds with the preprocessed landfire layers into a dataset.\n",
    "    Sets 'grid_mapping' to 'spatial_ref' in the encoding for every data variable in the process.\n",
    "    Returns: Stacked dataset.\n",
    "    \"\"\"\n",
    "    # Merge the two datasets\n",
    "    stacked_dataset = xr.merge([goes_ds, landfire_layers])\n",
    "\n",
    "    # Set 'grid_mapping' to 'spatial_ref' in the encoding for every data variable\n",
    "    for var in stacked_dataset.data_vars:\n",
    "        stacked_dataset[var].encoding['grid_mapping'] = 'spatial_ref'\n",
    "\n",
    "    # Optionally, delete 'goes_imager_projection' if it's no longer needed\n",
    "    if 'goes_imager_projection' in stacked_dataset:\n",
    "        del stacked_dataset['goes_imager_projection']\n",
    "\n",
    "    return stacked_dataset\n",
    "\n",
    "\n",
    "# def chunk_image(multiband_image, chunk_size=64):\n",
    "#     \"\"\"\n",
    "#     Splits the multiband image into chunks.\n",
    "#     Args:\n",
    "#         multiband_image (xarray.DataArray): The multiband image to be chunked.\n",
    "#         chunk_size (int): The size of the chunks. Default is 64.\n",
    "#     Returns:\n",
    "#         chunks (list): A list of xarray Datasets representing the chunks.\n",
    "#         spatial_info (list): A list of tuples representing the spatial information of each chunk.\n",
    "#     \"\"\"\n",
    "#     # Get the width and height of the image\n",
    "#     width = multiband_image.dims['x']\n",
    "#     height = multiband_image.dims['y']\n",
    "\n",
    "#     # Calculate the number of chunks in x and y direction\n",
    "#     nx, ny = width // chunk_size, height // chunk_size\n",
    "\n",
    "#     # Initialize a list to store the chunks\n",
    "#     chunks = []\n",
    "\n",
    "#     # Loop over the image\n",
    "#     for i in range(ny):\n",
    "#         for j in range(nx):\n",
    "#             # Define the slice\n",
    "#             y_slice = slice(i * chunk_size, (i + 1) * chunk_size)\n",
    "#             x_slice = slice(j * chunk_size, (j + 1) * chunk_size)\n",
    "\n",
    "#             # Extract the chunk across all bands\n",
    "#             chunk = multiband_image.isel(y=y_slice, x=x_slice)\n",
    "\n",
    "#             # Store the chunk and its spatial information\n",
    "#             chunks.append(chunk)\n",
    "#     return chunks\n",
    "\n",
    "def create_spatial_template(dataset):\n",
    "    \"\"\"\n",
    "    Creates a spatial template from the original dataset by keeping only one data variable\n",
    "    and setting its values to 0, while preserving spatial metadata. This step allows us to\n",
    "    Take the datavars out as a 21x64x64 numpy array, then add the model output back as 1x64x64\n",
    "    so that the spatial projection of the output is untouched.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: xarray.Dataset or rioxarray object with spatial dimensions and CRS information.\n",
    "    \n",
    "    Returns:\n",
    "    - template: xarray.Dataset with a single data variable filled with NaNs and original spatial metadata.\n",
    "    \"\"\"\n",
    "    # Clone the dataset to avoid modifying the original\n",
    "    template = dataset.copy()\n",
    "    \n",
    "    # Select the first data variable (assuming there's at least one)\n",
    "    first_var_name = list(template.data_vars)[0]\n",
    "    first_var = template[first_var_name]\n",
    "    \n",
    "    # Create a 0-filled template of the first variable\n",
    "    nan_template = xr.full_like(first_var, fill_value=np.nan)\n",
    "    \n",
    "    # Remove all data variables from the template\n",
    "    for var_name in list(template.data_vars):\n",
    "        del template[var_name]\n",
    "    \n",
    "    # Add the NaN-filled template variable back\n",
    "    template[first_var_name] = nan_template\n",
    "    \n",
    "    # Ensure the spatial metadata is preserved\n",
    "    # Note: This step might be redundant if the metadata is already attached to the coordinates\n",
    "    # and not the data variables themselves. However, it's a safeguard for maintaining CRS.\n",
    "    if hasattr(dataset, 'rio') and hasattr(dataset.rio, 'crs'):\n",
    "        template.rio.write_crs(dataset.rio.crs, inplace=True)\n",
    "    \n",
    "    return template\n",
    "\n",
    "def extract_data_as_array(dataset):\n",
    "    # Assuming time is the first dimension\n",
    "    return np.stack([dataset[var].values for var in dataset.data_vars], axis=0)\n",
    "\n",
    "def chunk_ndarray(arr, chunk_size=64):\n",
    "    # Assuming arr is in shape: (Variables, Time, Height, Width)\n",
    "    # and you want to chunk along the last two dimensions (Height, Width)\n",
    "    return np.array([arr[..., i:i+chunk_size, j:j+chunk_size] for i in range(0, arr.shape[-2], chunk_size) for j in range(0, arr.shape[-1], chunk_size)])\n",
    "\n",
    "def process_chunks(chunks):\n",
    "    # Example operation: sum along the first axis (simulating adding arrays together)\n",
    "    return np.sum(chunks, axis=1)\n",
    "\n",
    "def stitch_chunks(processed_chunks, original_shape):\n",
    "    # Assuming processed_chunks is a list of 2D arrays (Height, Width)\n",
    "    # and original_shape is the shape of the 2D plane of the original array (Height, Width)\n",
    "    stitched = np.zeros(original_shape)\n",
    "    chunk_size = processed_chunks[0].shape[0]\n",
    "    for k, chunk in enumerate(processed_chunks):\n",
    "        i, j = divmod(k, original_shape[1] // chunk_size)\n",
    "        stitched[i*chunk_size:(i+1)*chunk_size, j*chunk_size:(j+1)*chunk_size] = chunk\n",
    "    return stitched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_blobs = select_blobs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352036176_e20240352038555_c20240352039070.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352031176_e20240352033555_c20240352034074.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352026176_e20240352028549_c20240352029070.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352021176_e20240352023555_c20240352024065.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352016176_e20240352018555_c20240352019068.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352011176_e20240352013549_c20240352014072.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352006176_e20240352008556_c20240352009073.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/20/OR_ABI-L2-MCMIPC-M6_G16_s20240352001176_e20240352003549_c20240352004073.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/19/OR_ABI-L2-MCMIPC-M6_G16_s20240351956176_e20240351958549_c20240351959075.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/19/OR_ABI-L2-MCMIPC-M6_G16_s20240351951176_e20240351953561_c20240351954072.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/19/OR_ABI-L2-MCMIPC-M6_G16_s20240351946176_e20240351948555_c20240351949065.nc\n",
      "Opening: gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/035/19/OR_ABI-L2-MCMIPC-M6_G16_s20240351941176_e20240351943557_c20240351944076.nc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fs = create_fs()\n",
    "median_ds = create_median_image(selected_blobs, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: firenet_reference/combined_landfire.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_33404/8700081.py:109: SerializationWarning: saving variable y with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n",
      "/var/folders/53/g51yyrxd1gx8s6s6n03zzjbh0000gn/T/ipykernel_33404/8700081.py:109: SerializationWarning: saving variable x with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  dataset.to_netcdf(tmpfile.name)\n"
     ]
    }
   ],
   "source": [
    "landfire_layers = download_landfire_layers(fs)\n",
    "reprojected_median_ds = reproject_dataset(median_ds, landfire_layers)\n",
    "reprojected_median_ds = engineer_features(reprojected_median_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ds = stack_datasets(reprojected_median_ds, landfire_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_image(stacked_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ds.to_netcdf(\"stacked_dataset.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_datasets = [create_spatial_template(chunk) for chunk in chunks]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
