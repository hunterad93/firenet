{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import MultiPoint\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def get_firms_data(api_key, bbox, product, days_of_data = 1, date=None):\n",
    "    '''\n",
    "    Connect with FIRMS API to access data from a specified date, bbox, product, and range of days\n",
    "    and return it as a GeoDataFrame. If no date is specified, defaults to today.\n",
    "    \n",
    "    :param api_key: str, from NASA email, provided in cron job's request headers\n",
    "    :param bbox: str, bbox of the region of interest in the format \"minLongitude,minLatitude,maxLongitude,maxLatitude\", provided in cron job's request headers\n",
    "    :param date: str, date in '%Y-%m-%d' format. If not provided, defaults to today.\n",
    "    :return: GeoDataFrame of fire detection data with columns corresponding to the FIRMS API response\n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://firms.modaps.eosdis.nasa.gov/api/area/csv/'\n",
    "\n",
    "    # Simplify to get today's worth of data\n",
    "    date = datetime.now()  # Get today's date\n",
    "    formatted_date = date.strftime('%Y-%m-%d')  # Format date to '%Y-%m-%d'\n",
    "\n",
    "    url = f'{base_url}{api_key}/{product}/{bbox}/{days_of_data}/{formatted_date}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception if the request was unsuccessful\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while fetching data: {e}\")\n",
    "    else:\n",
    "        data = StringIO(response.text)  # Convert text response to file-like object\n",
    "        df = pd.read_csv(data)  # Read data into a DataFrame\n",
    "\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame, setting the geometry from the latitude and longitude columns\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_keep = ['latitude', 'longitude', 'confidence', 'geometry', 'acq_date', 'acq_time']\n",
    "    gdf = gdf[columns_to_keep]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def convert_modis_confidence_column(modis_gdf):\n",
    "\n",
    "def concat_firms_products(gdfs):\n",
    "\n",
    "\n",
    "def cluster_fires(gdf, eps=0.01, min_samples=1):\n",
    "    \"\"\"\n",
    "    Given a GeoDataFrame of fire points, create spatial clusters\n",
    "    :param gdf: GeoDataFrame of fire points\n",
    "    :param eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "    :param min_samples: The number of samples in a neighborhood for a point to be considered as a core point\n",
    "    :return: GeoDataFrame of fire points with an additional column 'label' indicating the cluster each point belongs to\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform DBSCAN clustering\n",
    "    coords = gdf[['longitude', 'latitude']].values\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(coords)\n",
    "\n",
    "    # Add cluster labels to the dataframe\n",
    "    gdf['label'] = db.labels_\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def filter_clusters(gdf, min_cluster_size=10, min_high_confidence=1):\n",
    "    \"\"\"\n",
    "    Filter out clusters that have fewer points, and fewer high confidence points, than the two thresholds\n",
    "    :param gdf: GeoDataFrame of fire points with 'label' column indicating the cluster each point belongs to\n",
    "    :param min_cluster_size: Minimum number of points in a cluster for it to be kept\n",
    "    :param min_high_confidence: Minimum number of high confidence points in a cluster for it to be kept\n",
    "    :return: GeoDataFrame of fire points in clusters that meet both thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of points in each cluster\n",
    "    cluster_counts = gdf['label'].value_counts()\n",
    "\n",
    "    # Count the number of high confidence points in each cluster\n",
    "    high_confidence_counts = gdf.loc[gdf['confidence'] == 'h']['label'].value_counts()\n",
    "\n",
    "    # Filter out small clusters and clusters with too few high confidence points\n",
    "    valid_clusters = cluster_counts[(cluster_counts >= min_cluster_size) & (high_confidence_counts >= min_high_confidence)].index\n",
    "    gdf = gdf[gdf['label'].isin(valid_clusters)]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def create_cluster_polygons(gdf):\n",
    "    \"\"\"\n",
    "    Given a GeoDataFrame of clustered fire points, create a polygon for each cluster\n",
    "    :param gdf: GeoDataFrame of fire points with 'label' column indicating the cluster each point belongs to\n",
    "    :return: Tuple containing the most frequently occurring acquisition date and a GeoJSON string where each feature represents a cluster and the geometry property contains the polygon around the cluster\n",
    "    \"\"\"\n",
    "    # Group the GeoDataFrame by the cluster labels\n",
    "    grouped = gdf.groupby('label')\n",
    "\n",
    "    # For each cluster, create a MultiPoint object from the fire points, then create a polygon from the convex hull of the points\n",
    "    polygons = grouped.apply(lambda df: MultiPoint(df.geometry.tolist()).convex_hull)\n",
    "\n",
    "    # Create a new GeoDataFrame from the polygons\n",
    "    polygon_gdf = gpd.GeoDataFrame({'geometry': polygons})\n",
    "\n",
    "    # Convert the GeoDataFrame to a GeoJSON string\n",
    "    polygon_geojson = polygon_gdf.to_json()\n",
    "\n",
    "    # Convert the most frequently occurring acquisition date to datetime\n",
    "    most_common_acq_date = pd.to_datetime(gdf['acq_date'].mode()[0])\n",
    "\n",
    "    return most_common_acq_date, polygon_geojson\n",
    "\n",
    "def upload_to_bigquery(acq_date, polygon_geojson):\n",
    "    \"\"\"\n",
    "    Uploads the polygon GeoJSON data to BigQuery.\n",
    "\n",
    "    :param acq_date: The most frequently occurring acquisition date. There will only ever be two dates in the GDF.\n",
    "    :param polygon_geojson: The GeoJSON string where each feature represents a cluster and the geometry property contains the polygon around the cluster.\n",
    "    \"\"\"\n",
    "    # Initialize a BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Specify your dataset and table\n",
    "    dataset_id = 'geojson_predictions'\n",
    "    table_id = 'viirs_mask'\n",
    "\n",
    "    # Get the table\n",
    "    table = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table)\n",
    "\n",
    "    # Convert acq_date to string for bigquery\n",
    "    acq_date = acq_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    # Prepare the row to be inserted\n",
    "    row = {\n",
    "        'prediction_date': acq_date,\n",
    "        'viirs_mask_geojson': polygon_geojson,\n",
    "        'datetime_added': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),  # UTC timestamp of the current moment\n",
    "    }\n",
    "\n",
    "    # Insert the row\n",
    "    errors = client.insert_rows_json(table, [row])\n",
    "\n",
    "    # Check if any errors occurred\n",
    "    if errors:\n",
    "        print('Errors:', errors)\n",
    "    else:\n",
    "        print('Row inserted successfully.')\n",
    "\n",
    "def VIIRS_GEOJSON_UPDATE(request):\n",
    "    # Get the request parameters from the cron job request that is sent to the cloud funtion\n",
    "    # The GCP cron job is where the API key and bbox are specified\n",
    "    request_json = request.get_json(silent=True)\n",
    "\n",
    "    api_key = request_json['api_key']\n",
    "    bbox = request_json['bbox']\n",
    "    # Delete request_json as it's no longer needed\n",
    "    del request_json\n",
    "\n",
    "    # Get the VIIRS data\n",
    "    viirs_data = get_viirs_data(api_key, bbox)\n",
    "\n",
    "    # Filter out points from the last 24 hours\n",
    "    viirs_data = filter_last_24_hours(viirs_data)\n",
    "\n",
    "    # Cluster the fire points\n",
    "    clustered_fires = cluster_fires(viirs_data)\n",
    "    # Delete viirs_data as it's no longer needed\n",
    "    del viirs_data\n",
    "\n",
    "    # Filter out small clusters and clusters with too few high confidence points\n",
    "    filtered_clusters = filter_clusters(clustered_fires)\n",
    "    # Delete clustered_fires as it's no longer needed\n",
    "    del clustered_fires\n",
    "\n",
    "    # Create a polygon for each cluster\n",
    "    acq_date, polygon_geojson = create_cluster_polygons(filtered_clusters)\n",
    "    # Delete filtered_clusters as it's no longer needed\n",
    "    del filtered_clusters\n",
    "\n",
    "    # Upload the polygon to BigQuery\n",
    "    upload_to_bigquery(acq_date, polygon_geojson)\n",
    "    # Delete acq_date and polygon_geojson as they're no longer needed\n",
    "    del acq_date, polygon_geojson\n",
    "\n",
    "    return 'Successfully processed and uploaded data', 200"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
