three_ps: a weekly progress report

Week 12: Reporting period 4/9/24-4/16/24

Progress: Spent time on peer review, code submissions, and made edits to the paper.

Problems: None

Plans: Finalize the paper and prepare for the presentation.

Hours: 8


Week 11: Reporting period 4/2/24-4/9/24

Progress: Revised the Capstone draft and worked on the digital product. Continued collaboration with populationexplorer.com

Problems: None

Plans: Review peer papers

Hours: 10

Week 10: Reporting period 3/26/24-4/2/24

Progress: Worked on the digital project and developed a cloud function for the populationexplorer.com collaboration.

Problems: None

Plans: Continue working with populationexplorer to integrate active fires with their maps, rewrite paper once feedback comes.

Hours: 20


Week 9: Reporting period 3/19/24-3/26/24

Progress: Completed the Capstone document.

Problems: No significant problems were encountered this week.

Plans: Prepare for presentation and work on digital product.

Hours: 8


Week 8: Reporting period 3/12/24-3/19/24

Progress: Continued writing the Capstone document, and further improvements were made to the digital product.

Problems: No significant problems were encountered this week.

Plans: Aim to finalize the draft over spring break.

Hours: 5


Week 7: Reporting period 3/5/24-3/12/24

Progress: Writing Capstone document, and improving digital product.

Problems: No significant problems were encountered this week.

Plans: Finish draft soon

Hours: 5


Week 6: Reporting period 2/27/24-3/5/24

Progress: I focused on integrating MODIS classification polygons into the pipeline. 
This addition will allow another high resolution comparison for Firenet predictions, following 
the recommendations from Corinne and James.

Problems: None

Plans: Probably going to refine retraining plan more.

Hours: 6


Week 5: Reporting period 2/20/24-2/27/24

Progress: Spent this week writing the first draft of my capstone report.

Problems: No problems encountered.

Plans: Plan to finish the draft in the next two weeks, and if I have time start putting together
the model retraining pipeline. Trying to prioritize having a good finalized project to present over
improving the model for the time being.

Hours: 12


Week 4: Reporting period 2/13/24-2/20/24

Progress: Dicussed how to retrain model with Sean - project collaborator, and made some changes to digital project website.

Problems: No problems just juggling other tasks

Plans: Going to start writing paper before revisiting retraining

Hours: 5


Week 3: Reporting period 2/6/24-2/13/24

Progress: Successfully managed to get the firenet inference pipeline fully operational, creating a seamless automated
 cloud pipeline. This pipeline is now composed of two cloud functions; the first one, equipped with 16GB of RAM,
  is responsible for generating the input .nc file. The second function, a 32GB RAM containerized function, runs
the neural net processing and uploads the resulting geojson to Google BigQuery (GBQ). This pretty much is the completion
of my project as I laid it out, but there are some issues I might fix time permitting.

Problems: Main problem is that the model needs to be retrained. Secondary problem is that the cloud function takes way more
ram than expected, I think there is some issue with the containerized function having a memory leak.

Plans: I might work on retraining, I will probably leave the memory leak issue alone for a while and only worry about that once
the model predictions seem more accurate.

Hours: 25


Week2: Reporting period 1/30/24-2/6/24

Progress: Dealt with last week's data problems by using a small northwest USA region to reproject everything to. Researched
adding MODIS data to the map, it is similar to VIIRS so could re use alot of code. Tightened up some documentation.

Finally discovered problem causing spatial metadata inconsistencies. The .encoding property of xarray dataset vars
kept not getting changed, then rioxarray.reproject methods would fail to correctly reproject. Solving this was a huge
breakthrough and I think I finally understand the complex nested structure of netcdf files.

After the above breakthrough I was able to pass things through in the correct format to the neural net. I developed
a function to preserve spatial metadata of the chunks that get passed which meant I could georeference the predictions which
came out of the neural net.

Problems: Something looks wrong with the predictions, but they are at least in the correct spatial area. Im sure this will
be another problem to solve.

Plans: Meet with Sean to view the first predictions coming out and understand why they look so wrong.
Hours: 20

Week 1: Reporting period 1/23/24-1/30/24

Progress: Developed a pipeline to download, reproject, and resample GOES MCIMPC raster data, fixing bugs with spatial metadata not being read and saved during processing steps.
Problems: Data is excessively large ~ 48gb for each input raster stack. Running this in cloud can get very expensive.
Plans: Build the data pipeline for a lower-resolution version of the data, this will mean the predictions will be very innacurate but may be worth building as a prototype. Other option is to build it with full resolution but run it very infrequently depending on actual cost.
Hours: 10

Pre week 1 progress 12/23/23-1/23/24

Progress:
1. Developed and deployed a Google Cloud Function for pulling GOES FDCC (hot spot characterization) raster data and processing it into geojson, then uploading the geojson to a BigQuery database.
2. Developed another cloud function that accessed VIIRS hotspot characterization, used DBSCAN algorithm to identify clusters, and uploaded geojson polygons to the BigQuery database.
3. Developed and deployed a flask server to display geojson predictions with leaflet via uploading from the BigQuery database.
4. Began work on a website - my digital project deliverable - that will introduce the project and display the leaflet map.
5. Began work on the firenet inference pipeline, this will download GOES data, use it to output fire predictions as geojson and upload to the BigQuery database.

Problems:
1. Learning curve with GCP, moving working Python scripts into the cloud and properly managing roles + credentials to give functions proper permissions to access databases.
2. General issues with raster data, especially when needing to spatially match up differing satellite products in the firenet inference pipeline.

Plans:
1. Finish developing the firenet inference pipeline.
2. Continue work on digital product deliverable website.
