three_ps: a weekly progress report

Week 5: Reporting period 2/20/24-2/27/24

Progress: Spent this week writing the first draft of my capstone report.

Problems: No problems encountered.

Plans: Plan to finish the draft in the next two weeks, and if I have time start putting together
the model retraining pipeline. Trying to prioritize having a good finalized project to present over
improving the model for the time being.

Hours: 12


Week 4: Reporting period 2/13/24-2/20/24

Progress: Dicussed how to retrain model with Sean - project collaborator, and made some changes to digital project website.

Problems: No problems just juggling other tasks

Plans: Going to start writing paper before revisiting retraining

Hours: 5


Week 3: Reporting period 2/6/24-2/13/24

Progress: Successfully managed to get the firenet inference pipeline fully operational, creating a seamless automated
 cloud pipeline. This pipeline is now composed of two cloud functions; the first one, equipped with 16GB of RAM,
  is responsible for generating the input .nc file. The second function, a 32GB RAM containerized function, runs
the neural net processing and uploads the resulting geojson to Google BigQuery (GBQ). This pretty much is the completion
of my project as I laid it out, but there are some issues I might fix time permitting.

Problems: Main problem is that the model needs to be retrained. Secondary problem is that the cloud function takes way more
ram than expected, I think there is some issue with the containerized function having a memory leak.

Plans: I might work on retraining, I will probably leave the memory leak issue alone for a while and only worry about that once
the model predictions seem more accurate.

Hours: 25


Week2: Reporting period 1/30/24-2/6/24

Progress: Dealt with last week's data problems by using a small northwest USA region to reproject everything to. Researched
adding MODIS data to the map, it is similar to VIIRS so could re use alot of code. Tightened up some documentation.

Finally discovered problem causing spatial metadata inconsistencies. The .encoding property of xarray dataset vars
kept not getting changed, then rioxarray.reproject methods would fail to correctly reproject. Solving this was a huge
breakthrough and I think I finally understand the complex nested structure of netcdf files.

After the above breakthrough I was able to pass things through in the correct format to the neural net. I developed
a function to preserve spatial metadata of the chunks that get passed which meant I could georeference the predictions which
came out of the neural net.

Problems: Something looks wrong with the predictions, but they are at least in the correct spatial area. Im sure this will
be another problem to solve.

Plans: Meet with Sean to view the first predictions coming out and understand why they look so wrong.
Hours: 20

Week 1: Reporting period 1/23/24-1/30/24

Progress: Developed a pipeline to download, reproject, and resample GOES MCIMPC raster data, fixing bugs with spatial metadata not being read and saved during processing steps.
Problems: Data is excessively large ~ 48gb for each input raster stack. Running this in cloud can get very expensive.
Plans: Build the data pipeline for a lower-resolution version of the data, this will mean the predictions will be very innacurate but may be worth building as a prototype. Other option is to build it with full resolution but run it very infrequently depending on actual cost.
Hours: 10

Pre week 1 progress 12/23/23-1/23/24

Progress:
1. Developed and deployed a Google Cloud Function for pulling GOES FDCC (hot spot characterization) raster data and processing it into geojson, then uploading the geojson to a BigQuery database.
2. Developed another cloud function that accessed VIIRS hotspot characterization, used DBSCAN algorithm to identify clusters, and uploaded geojson polygons to the BigQuery database.
3. Developed and deployed a flask server to display geojson predictions with leaflet via uploading from the BigQuery database.
4. Began work on a website - my digital project deliverable - that will introduce the project and display the leaflet map.
5. Began work on the firenet inference pipeline, this will download GOES data, use it to output fire predictions as geojson and upload to the BigQuery database.

Problems:
1. Learning curve with GCP, moving working Python scripts into the cloud and properly managing roles + credentials to give functions proper permissions to access databases.
2. General issues with raster data, especially when needing to spatially match up differing satellite products in the firenet inference pipeline.

Plans:
1. Finish developing the firenet inference pipeline.
2. Continue work on digital product deliverable website.
